{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy is like the Swiss Army knife of NLP, while Transformers is more akin to a sledge hammer.\n",
    "\n",
    "SpaCy is fast and lightweight. Transformers (ie. Sentence transformer) let’s you use state of the art stuff, but the trade off is usually in terms of slower runtime at inference and larger memory usage.\n",
    "\n",
    "Another important distinction is that SpaCy has tools for more linguistics-focused tasks, such as dependency parsing, and annotations. While transformers has tools for tasks that span beyond just NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# the problem with fuzz is that it does not capture sematic meaning -> good ratio can be very bad since wording is key in TORs\n",
    "str1 = 'Oracle             database'\n",
    "str2 = 'Oracle database'\n",
    "display(fuzz.token_sort_ratio(str1, str2)) # token based -> order does not matter as much as long as words are the same\n",
    "display(fuzz.ratio(str1, str2)) # Order matters -> whitespace also effect the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.8878823518753052\n"
     ]
    }
   ],
   "source": [
    "# test using sentence models -> pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define the two sentences\n",
    "sentence1 = 'I want to really eat some ice cream at the store'\n",
    "sentence2 = 'I want to really not eat some ice cream at the storesssssssssss'\n",
    "\n",
    "# Generate embeddings for each sentence\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity between the embeddings\n",
    "cosine_score = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(\"Cosine similarity:\", cosine_score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lang': 'en', 'name': 'core_web_trf', 'version': '3.8.0', 'description': \"English transformer pipeline (Transformer(name='roberta-base', piece_encoder='byte-bpe', stride=104, type='roberta', width=768, window=144, vocab_size=50265)). Components: transformer, tagger, parser, ner, attribute_ruler, lemmatizer.\", 'author': 'Explosion', 'email': 'contact@explosion.ai', 'url': 'https://explosion.ai', 'license': 'MIT', 'spacy_version': '>=3.8.0,<3.9.0', 'spacy_git_version': '5010fcbd3', 'vectors': {'width': 0, 'vectors': 0, 'keys': 0, 'name': None, 'mode': 'default'}, 'labels': {'transformer': [], 'tagger': ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '``'], 'parser': ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp'], 'attribute_ruler': [], 'lemmatizer': [], 'ner': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']}, 'pipeline': ['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'], 'components': ['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'], 'disabled': [], 'performance': {'token_acc': 0.9986194413, 'token_p': 0.9956819193, 'token_r': 0.9957659295000001, 'token_f': 0.9957239226000001, 'tag_acc': 0.9811806415000001, 'sents_p': 0.9411328038000001, 'sents_r': 0.8363348506, 'sents_f': 0.8856444289000001, 'dep_uas': 0.9518836807000001, 'dep_las': 0.9384315055, 'dep_las_per_type': {'prep': {'p': 0.9229283442, 'r': 0.92464428, 'f': 0.9237855153000001}, 'det': {'p': 0.9902952210000001, 'r': 0.9902144663, 'f': 0.990254842}, 'pobj': {'p': 0.9839824121, 'r': 0.9842528961, 'f': 0.9841176355000001}, 'nsubj': {'p': 0.9830732029, 'r': 0.9796276013, 'f': 0.9813473777}, 'aux': {'p': 0.9872967931000001, 'r': 0.9894062138, 'f': 0.9883503779}, 'advmod': {'p': 0.8977991399, 'r': 0.8957597173, 'f': 0.8967782691}, 'relcl': {'p': 0.8814229249000001, 'r': 0.8900580552, 'f': 0.8857194439}, 'root': {'p': 0.9662979734, 'r': 0.8586978033, 'f': 0.9093258819000001}, 'xcomp': {'p': 0.9405833633, 'r': 0.9375448672000001, 'f': 0.9390616574}, 'amod': {'p': 0.9467031535, 'r': 0.9413670230000001, 'f': 0.9440275477000001}, 'compound': {'p': 0.9513269402000001, 'r': 0.9502673201, 'f': 0.9507968349}, 'poss': {'p': 0.9871330921, 'r': 0.9883252818, 'f': 0.9877288272}, 'ccomp': {'p': 0.8349658104000001, 'r': 0.9201629328, 'f': 0.8754965604}, 'attr': {'p': 0.9531897266, 'r': 0.9676198486, 'f': 0.9603505843000001}, 'case': {'p': 0.9890547264, 'r': 0.9949949950000001, 'f': 0.9920159681}, 'mark': {'p': 0.949343832, 'r': 0.958399576, 'f': 0.9538502110000001}, 'intj': {'p': 0.6096131301000001, 'r': 0.7619047619, 'f': 0.6773038098}, 'advcl': {'p': 0.8101686254, 'r': 0.7985394107, 'f': 0.8043119848}, 'cc': {'p': 0.8934728332, 'r': 0.8988159311, 'f': 0.8961364178000001}, 'neg': {'p': 0.9633718013, 'r': 0.9633718013, 'f': 0.9633718013}, 'conj': {'p': 0.8668266347, 'r': 0.9094914401, 'f': 0.8876466613}, 'nsubjpass': {'p': 0.9514415781000001, 'r': 0.9646153846000001, 'f': 0.9579831933}, 'auxpass': {'p': 0.9624329159, 'r': 0.9804100228, 'f': 0.9713382984000001}, 'dobj': {'p': 0.9721448468, 'r': 0.9733843334000001, 'f': 0.9727641953}, 'nummod': {'p': 0.9454773869, 'r': 0.9502525253, 'f': 0.9478589421}, 'npadvmod': {'p': 0.8512611276000001, 'r': 0.8152753108, 'f': 0.8328796952}, 'prt': {'p': 0.8621553885000001, 'r': 0.9247311828, 'f': 0.8923476005000001}, 'pcomp': {'p': 0.9290231904, 'r': 0.9257703081, 'f': 0.9273938969000001}, 'expl': {'p': 0.9935760171, 'r': 0.9935760171, 'f': 0.9935760171}, 'acl': {'p': 0.8420767983, 'r': 0.8494271686, 'f': 0.845736013}, 'agent': {'p': 0.9430051813, 'r': 0.9784946237000001, 'f': 0.9604221636}, 'dative': {'p': 0.8163716814, 'r': 0.8463302752, 'f': 0.8310810811}, 'acomp': {'p': 0.9561975769000001, 'r': 0.9306122449, 'f': 0.943231441}, 'dep': {'p': 0.4446337308, 'r': 0.4237012987, 'f': 0.43391521200000005}, 'csubj': {'p': 0.9053254438, 'r': 0.9053254438, 'f': 0.9053254438}, 'quantmod': {'p': 0.874251497, 'r': 0.8302193339, 'f': 0.8516666667}, 'nmod': {'p': 0.8405292479, 'r': 0.7355271176, 'f': 0.7845303867000001}, 'appos': {'p': 0.7903822441, 'r': 0.8342733189, 'f': 0.8117349092}, 'predet': {'p': 0.864, 'r': 0.9270386266, 'f': 0.8944099379}, 'preconj': {'p': 0.6966292135000001, 'r': 0.7209302326, 'f': 0.7085714286}, 'oprd': {'p': 0.8840125392, 'r': 0.8417910448, 'f': 0.8623853211}, 'parataxis': {'p': 0.5677179963, 'r': 0.6637744035, 'f': 0.612}, 'meta': {'p': 0.23178807950000002, 'r': 0.6730769231, 'f': 0.34482758620000004}, 'csubjpass': {'p': 0.8333333333, 'r': 0.8333333333, 'f': 0.8333333333}}, 'ents_p': 0.8972927158, 'ents_r': 0.9012920673, 'ents_f': 0.8992879450000001, 'ents_per_type': {'DATE': {'p': 0.8915700407, 'r': 0.9031746032, 'f': 0.8973348052000001}, 'GPE': {'p': 0.9563739377, 'r': 0.9417015342, 'f': 0.948981026}, 'ORDINAL': {'p': 0.7994011976000001, 'r': 0.8291925466000001, 'f': 0.8140243902000001}, 'ORG': {'p': 0.8959323825000001, 'r': 0.8992576882000001, 'f': 0.8975919555}, 'FAC': {'p': 0.5815217391, 'r': 0.8230769231, 'f': 0.6815286624}, 'QUANTITY': {'p': 0.7558139535, 'r': 0.7142857143, 'f': 0.7344632768}, 'LOC': {'p': 0.8317460317, 'r': 0.8343949045, 'f': 0.8330683625}, 'CARDINAL': {'p': 0.8628266033, 'r': 0.8638525565, 'f': 0.8633392751000001}, 'PERSON': {'p': 0.9248753117, 'r': 0.9683420366000001, 'f': 0.9461096939}, 'NORP': {'p': 0.9324758842, 'r': 0.928, 'f': 0.9302325581}, 'LAW': {'p': 0.5915492958, 'r': 0.65625, 'f': 0.6222222222}, 'TIME': {'p': 0.7635869565000001, 'r': 0.8216374269000001, 'f': 0.7915492958}, 'MONEY': {'p': 0.9289099526000001, 'r': 0.9256198347000001, 'f': 0.9272619752000001}, 'EVENT': {'p': 0.8167938931000001, 'r': 0.6149425287, 'f': 0.7016393443000001}, 'PRODUCT': {'p': 0.627027027, 'r': 0.5497630332, 'f': 0.5858585859000001}, 'WORK_OF_ART': {'p': 0.6582278481, 'r': 0.5360824742, 'f': 0.5909090909}, 'PERCENT': {'p': 0.923566879, 'r': 0.8882082695, 'f': 0.9055425449000001}, 'LANGUAGE': {'p': 1.0, 'r': 0.75, 'f': 0.8571428571}}, 'speed': 4179.5216564566}, 'sources': [{'name': 'OntoNotes 5', 'url': 'https://catalog.ldc.upenn.edu/LDC2013T19', 'license': 'commercial (licensed by Explosion)', 'author': 'Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Robert Belvin, Ann Houston'}, {'name': 'ClearNLP Constituent-to-Dependency Conversion', 'url': 'https://github.com/clir/clearnlp-guidelines/blob/master/md/components/dependency_conversion.md', 'license': 'Citation provided for reference, no code packaged with model', 'author': 'Emory University'}, {'name': 'WordNet 3.0', 'url': 'https://wordnet.princeton.edu/', 'author': 'Princeton University', 'license': 'WordNet 3.0 License'}, {'name': 'roberta-base', 'author': 'Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov', 'url': 'https://github.com/pytorch/fairseq/tree/master/examples/roberta', 'license': ''}], 'requirements': ['spacy-curated-transformers>=0.2.2,<1.0.0']}\n"
     ]
    }
   ],
   "source": [
    "# Create spacy nlp object\n",
    "# load en_core_web_md (small model), en_core_web_lg (large model), en_core_web_trf (largest)\n",
    "# pip uninstall en-core-web-lg\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "print(nlp.meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statement accuracy rate, compare between sentence transformer vs spacy vs fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 11/11 [00:01<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([341, 384])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # for data manipulation\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Import the two excel file - input file and reference file\n",
    "df_main = pd.read_excel('Excel_file/Main.xlsx')\n",
    "df_compare = pd.read_excel('Excel_file/Compare.xlsx')\n",
    "\n",
    "# Import thai compatible model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Encode all statements from Main.xlsx as a single batch\n",
    "main_statements = df_main['Statement'].tolist()\n",
    "main_embeddings = model.encode(main_statements, convert_to_tensor=True, show_progress_bar=True)\n",
    "print(type(main_embeddings))\n",
    "main_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # for caching main embeddings\n",
    "\n",
    "# testing pickle, pk1 is pickle file, can be any file type really but pk1 just to demonstrate\n",
    "student_names = ['Kay','Bob','Elena','Jane','Kyle']\n",
    "with open('student_file.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(student_names, f) # serialize the list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kay', 'Bob', 'Elena', 'Jane', 'Kyle']\n"
     ]
    }
   ],
   "source": [
    "with open('student_file.pkl', 'rb') as f:  # open a text file\n",
    "    list_name = pickle.load(f) # deserialize the list\n",
    "f.close()\n",
    "print(list_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TOR comply number                               TOR comply statement\n",
      "5                4.2  สามารถเลือกทำงานบนระบบปฏิบัติการ Windows หรือ ...\n",
      "6                4.3  เป็นฐานข้อมูลที่มีระบบ Lock ข้อมูลในระดับ Row ...\n",
      "7                4.4  มีคุณสมบัติในการทำ Multi-Version Read Consiste...\n",
      "8                4.5  สามารถทำการเก็บข้อมูลและแสดงผลได้ทั้งภาษาไทยแล...\n",
      "9                4.6  มีการทำงานแบบ Machine Learning เพื่อช่วยเพิ่มป...\n",
      "10               4.7  มีการทำงานแบบ Query Optimization และสามารถทำงา...\n",
      "11               4.8  สามารถรองรับการจัดเก็บข้อมูลในรูปแบบ JSON โดยส...\n",
      "12               4.9       สามารถทำงานในรูปแบบระบบฐานข้อมูลแบบ Graph ได\n",
      "13               4.1  มีเครื่องมือรองรับในการจัดการระบบไฟล์สำหรับไฟล...\n",
      "14               NaN  4.10.1 รองรับการช่วยกระจาย I/O ไปยังดิสก์ข้อมู...\n",
      "15               NaN  4.10.2 รองรับการเพิ่มหรือลดจำนวน disk ได้โดยไม...\n",
      "16               NaN  4.10.3 รองรับการจัดเรียงการกระจายของข้อมูลใหม่...\n",
      "17               NaN  4.10.4 รองรับการ Mirror Resync ข้อมูลระหว่าง D...\n",
      "18              4.11     รองรับการทำงานในลักษณะ Cluster (Active/Active)\n",
      "19              4.12  มาพร้อมกับเครื่องมือในการสร้าง Web Application...\n",
      "20              4.13  สามารถทำงานแบบ Multi-Tenant ได้ไม่น้อยกว่า 3Te...\n",
      "21              4.14  ต้องสนับสนุน เน็ตเวิร์คโปรโตคอลแบบ TCP/IP เป็น...\n",
      "22              4.15  มีลิขสิทธิ์ใช้งานถูกต้องตามกฎหมายแบบไม่จำกัดจำ...\n"
     ]
    }
   ],
   "source": [
    "# selecting excel test\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def excel_cell_to_indices(cell_str):\n",
    "    \"\"\"\n",
    "    Converts an Excel cell address (e.g., \"A5\") to zero-based (row, column) indices.\n",
    "    \"\"\"\n",
    "    match = re.match(r\"([A-Za-z]+)([0-9]+)\", cell_str)\n",
    "    if not match:\n",
    "        raise ValueError(\"Invalid cell format: \" + cell_str)\n",
    "    col_str, row_str = match.groups()\n",
    "    # Convert letters to a zero-based column index:\n",
    "    col_idx = 0\n",
    "    for char in col_str.upper():\n",
    "        col_idx = col_idx * 26 + (ord(char) - ord('A') + 1)\n",
    "    col_idx -= 1  # adjust to zero-based index\n",
    "    row_idx = int(row_str) - 1  # adjust to zero-based index\n",
    "    return row_idx, col_idx\n",
    "\n",
    "def slice_excel_by_cells(df, num_start, num_end, stmt_start, stmt_end):\n",
    "    \"\"\"\n",
    "    Extracts two series from the DataFrame based on provided Excel cell ranges.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame read from the Excel file.\n",
    "        num_start (str): Starting cell for TOR comply numbers (e.g., \"A5\").\n",
    "        num_end (str): Ending cell for TOR comply numbers (e.g., \"A23\").\n",
    "        stmt_start (str): Starting cell for TOR comply statements (e.g., \"B5\").\n",
    "        stmt_end (str): Ending cell for TOR comply statements (e.g., \"B23\").\n",
    "    \n",
    "    Returns:\n",
    "        (pd.Series, pd.Series): Two series, one for numbers and one for statements.\n",
    "    \"\"\"\n",
    "    num_start_row, num_start_col = excel_cell_to_indices(num_start)\n",
    "    num_end_row, _ = excel_cell_to_indices(num_end)  # Column should be same as start for numbers\n",
    "    stmt_start_row, stmt_start_col = excel_cell_to_indices(stmt_start)\n",
    "    stmt_end_row, _ = excel_cell_to_indices(stmt_end)  # Column should be same as start for statements\n",
    "    \n",
    "    # Slicing includes the ending row so add 1 (pandas slicing is end-exclusive)\n",
    "    numbers = df.iloc[num_start_row:num_end_row+1, num_start_col]\n",
    "    statements = df.iloc[stmt_start_row:stmt_end_row+1, stmt_start_col]\n",
    "    return numbers, statements\n",
    "\n",
    "# Example usage:\n",
    "# Read the Excel file (adjust header settings if needed)\n",
    "df = pd.read_excel(\"Excel_file/Unformat_test.xlsx\", header=None)\n",
    "\n",
    "# Dynamically select ranges using Excel cell notation.\n",
    "tor_numbers, tor_statements = slice_excel_by_cells(df, \"A6\", \"A23\", \"B6\", \"B23\")\n",
    "\n",
    "# Combine into a new DataFrame with proper column names\n",
    "result_df = pd.DataFrame({\n",
    "    \"TOR comply number\": tor_numbers,\n",
    "    \"TOR comply statement\": tor_statements\n",
    "})\n",
    "\n",
    "print(result_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
