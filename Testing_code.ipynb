{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy is like the Swiss Army knife of NLP, while Transformers is more akin to a sledge hammer.\n",
    "\n",
    "SpaCy is fast and lightweight. Transformers (ie. Sentence transformer) let’s you use state of the art stuff, but the trade off is usually in terms of slower runtime at inference and larger memory usage.\n",
    "\n",
    "Another important distinction is that SpaCy has tools for more linguistics-focused tasks, such as dependency parsing, and annotations. While transformers has tools for tasks that span beyond just NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# the problem with fuzz is that it does not capture sematic meaning -> good ratio can be very bad since wording is key in TORs\n",
    "#str1 = 'สามารถติดตั้งบนระบบปฏิบัติการต่าง ๆ เช่น Windows Server, Linux, Unix ได้เป็นอย่างน้อย'\n",
    "#str2 = 'สามารถเลือกทำงานบนระบบปฏิบัติการ Windows หรือ ระบบปฏิบัติการ UNIX'\n",
    "\n",
    "#str1 = 'สามารถใช้งาน Lock ข้อมูลในระดับแถว (Row Level Locking) ได้อัตโนมัติ โดยไม่ต้องมีการพัฒนาโปรแกรมเพิ่มเติม'\n",
    "#str2 = 'เป็นฐานข้อมูลที่มีระบบ Lock ข้อมูลในระดับ Row Level Locking จริง ๆ ซึ่ง Database Engine กระทำได้เอง โดยต้องไม่มีการเขียนโปรแกรมเพิ่มเติม'\n",
    "\n",
    "str1 = 'Im very happy right now, so thats the best right'\n",
    "str2 = 'Im very sad right now, so thats the best right'\n",
    "\n",
    "display(fuzz.token_sort_ratio(str1, str2)) # token based -> order does not matter as much as long as words are the same\n",
    "display(fuzz.ratio(str1, str2)) # Order matters -> whitespace also effect the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzz does not give good result for similar text - if not excat match. Compare this to sentencetransformer below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.7313247919082642\n"
     ]
    }
   ],
   "source": [
    "# test using sentence models -> pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# load a pre-trained model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Define the two sentences\n",
    "sentence1 = 'Im very happy right now, so thats the best right'\n",
    "sentence2 = 'Im very sad right now, so thats the best right'\n",
    "\n",
    "# Generate embeddings for each sentence\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity between the embeddings\n",
    "cosine_score = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(\"Cosine similarity:\", cosine_score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.8878823518753052\n"
     ]
    }
   ],
   "source": [
    "# test using sentence models -> pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define the two sentences\n",
    "sentence1 = 'I want to really eat some ice cream at the store'\n",
    "sentence2 = 'I want to really not eat some ice cream at the storesssssssssss'\n",
    "\n",
    "# Generate embeddings for each sentence\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity between the embeddings\n",
    "cosine_score = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(\"Cosine similarity:\", cosine_score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_trf'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create spacy nlp object\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# load en_core_web_md (small model), en_core_web_lg (large model), en_core_web_trf (largest)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# pip uninstall en-core-web-lg\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#nlp = spacy.load(\"en_core_web_lg\")\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_trf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(nlp\u001b[38;5;241m.\u001b[39mmeta)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_trf'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Create spacy nlp object\n",
    "# load en_core_web_md (small model), en_core_web_lg (large model), en_core_web_trf (largest)\n",
    "# pip uninstall en-core-web-lg\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "print(nlp.meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statement accuracy rate, compare between sentence transformer vs spacy vs fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 11/11 [00:01<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([341, 384])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # for data manipulation\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Import the two excel file - input file and reference file\n",
    "df_main = pd.read_excel('Excel_file/Main.xlsx')\n",
    "df_compare = pd.read_excel('Excel_file/Compare.xlsx')\n",
    "\n",
    "# Import thai compatible model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Encode all statements from Main.xlsx as a single batch\n",
    "main_statements = df_main['Statement'].tolist()\n",
    "main_embeddings = model.encode(main_statements, convert_to_tensor=True, show_progress_bar=True)\n",
    "print(type(main_embeddings))\n",
    "main_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # for caching main embeddings\n",
    "\n",
    "# testing pickle, pk1 is pickle file, can be any file type really but pk1 just to demonstrate\n",
    "student_names = ['Kay','Bob','Elena','Jane','Kyle']\n",
    "with open('student_file.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(student_names, f) # serialize the list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kay', 'Bob', 'Elena', 'Jane', 'Kyle']\n"
     ]
    }
   ],
   "source": [
    "with open('student_file.pkl', 'rb') as f:  # open a text file\n",
    "    list_name = pickle.load(f) # deserialize the list\n",
    "f.close()\n",
    "print(list_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TOR comply number                               TOR comply statement\n",
      "5                4.2  สามารถเลือกทำงานบนระบบปฏิบัติการ Windows หรือ ...\n",
      "6                4.3  เป็นฐานข้อมูลที่มีระบบ Lock ข้อมูลในระดับ Row ...\n",
      "7                4.4  มีคุณสมบัติในการทำ Multi-Version Read Consiste...\n",
      "8                4.5  สามารถทำการเก็บข้อมูลและแสดงผลได้ทั้งภาษาไทยแล...\n",
      "9                4.6  มีการทำงานแบบ Machine Learning เพื่อช่วยเพิ่มป...\n",
      "10               4.7  มีการทำงานแบบ Query Optimization และสามารถทำงา...\n",
      "11               4.8  สามารถรองรับการจัดเก็บข้อมูลในรูปแบบ JSON โดยส...\n",
      "12               4.9       สามารถทำงานในรูปแบบระบบฐานข้อมูลแบบ Graph ได\n",
      "13               4.1  มีเครื่องมือรองรับในการจัดการระบบไฟล์สำหรับไฟล...\n",
      "14               NaN  4.10.1 รองรับการช่วยกระจาย I/O ไปยังดิสก์ข้อมู...\n",
      "15               NaN  4.10.2 รองรับการเพิ่มหรือลดจำนวน disk ได้โดยไม...\n",
      "16               NaN  4.10.3 รองรับการจัดเรียงการกระจายของข้อมูลใหม่...\n",
      "17               NaN  4.10.4 รองรับการ Mirror Resync ข้อมูลระหว่าง D...\n",
      "18              4.11     รองรับการทำงานในลักษณะ Cluster (Active/Active)\n",
      "19              4.12  มาพร้อมกับเครื่องมือในการสร้าง Web Application...\n",
      "20              4.13  สามารถทำงานแบบ Multi-Tenant ได้ไม่น้อยกว่า 3Te...\n",
      "21              4.14  ต้องสนับสนุน เน็ตเวิร์คโปรโตคอลแบบ TCP/IP เป็น...\n",
      "22              4.15  มีลิขสิทธิ์ใช้งานถูกต้องตามกฎหมายแบบไม่จำกัดจำ...\n"
     ]
    }
   ],
   "source": [
    "# selecting excel test\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def excel_cell_to_indices(cell_str):\n",
    "    \"\"\"\n",
    "    Converts an Excel cell address (e.g., \"A5\") to zero-based (row, column) indices.\n",
    "    \"\"\"\n",
    "    match = re.match(r\"([A-Za-z]+)([0-9]+)\", cell_str)\n",
    "    if not match:\n",
    "        raise ValueError(\"Invalid cell format: \" + cell_str)\n",
    "    col_str, row_str = match.groups()\n",
    "    # Convert letters to a zero-based column index:\n",
    "    col_idx = 0\n",
    "    for char in col_str.upper():\n",
    "        col_idx = col_idx * 26 + (ord(char) - ord('A') + 1)\n",
    "    col_idx -= 1  # adjust to zero-based index\n",
    "    row_idx = int(row_str) - 1  # adjust to zero-based index\n",
    "    return row_idx, col_idx\n",
    "\n",
    "def slice_excel_by_cells(df, num_start, num_end, stmt_start, stmt_end):\n",
    "    \"\"\"\n",
    "    Extracts two series from the DataFrame based on provided Excel cell ranges.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame read from the Excel file.\n",
    "        num_start (str): Starting cell for TOR comply numbers (e.g., \"A5\").\n",
    "        num_end (str): Ending cell for TOR comply numbers (e.g., \"A23\").\n",
    "        stmt_start (str): Starting cell for TOR comply statements (e.g., \"B5\").\n",
    "        stmt_end (str): Ending cell for TOR comply statements (e.g., \"B23\").\n",
    "    \n",
    "    Returns:\n",
    "        (pd.Series, pd.Series): Two series, one for numbers and one for statements.\n",
    "    \"\"\"\n",
    "    num_start_row, num_start_col = excel_cell_to_indices(num_start)\n",
    "    num_end_row, _ = excel_cell_to_indices(num_end)  # Column should be same as start for numbers\n",
    "    stmt_start_row, stmt_start_col = excel_cell_to_indices(stmt_start)\n",
    "    stmt_end_row, _ = excel_cell_to_indices(stmt_end)  # Column should be same as start for statements\n",
    "    \n",
    "    # Slicing includes the ending row so add 1 (pandas slicing is end-exclusive)\n",
    "    numbers = df.iloc[num_start_row:num_end_row+1, num_start_col]\n",
    "    statements = df.iloc[stmt_start_row:stmt_end_row+1, stmt_start_col]\n",
    "    return numbers, statements\n",
    "\n",
    "# Example usage:\n",
    "# Read the Excel file (adjust header settings if needed)\n",
    "df = pd.read_excel(\"Excel_file/Unformat_test.xlsx\", header=None)\n",
    "\n",
    "# Dynamically select ranges using Excel cell notation.\n",
    "tor_numbers, tor_statements = slice_excel_by_cells(df, \"A6\", \"A23\", \"B6\", \"B23\")\n",
    "\n",
    "# Combine into a new DataFrame with proper column names\n",
    "result_df = pd.DataFrame({\n",
    "    \"TOR comply number\": tor_numbers,\n",
    "    \"TOR comply statement\": tor_statements\n",
    "})\n",
    "\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Model: 100%|██████████| 1/1 [00:03<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "Loaded cache file for main embeddings!\n",
      "Encoding compare statements in batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarity scores in a single pass...\n",
      "Total time: 3.595 seconds taken\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# single batch code - LLM generated\n",
    "import pandas as pd  # for data manipulation\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load the two Excel files\n",
    "df_main = pd.read_excel('Excel_file/Main.xlsx')\n",
    "df_compare = pd.read_excel('Excel_file/Compare.xlsx')\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "print(\"Start loading model...\")\n",
    "with tqdm(total=1, desc=\"Loading Model\") as pbar:\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    pbar.update(1)\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Cache file for main embeddings\n",
    "cache_file = 'main_embeddings.pkl'\n",
    "\n",
    "# Either load cached embeddings or create them if they don't exist\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        main_embeddings = pickle.load(f)\n",
    "    print(\"Loaded cache file for main embeddings!\")\n",
    "else:\n",
    "    print(\"Start embedding main statements...\")\n",
    "    main_statements = df_main['Statement'].tolist()\n",
    "    main_embeddings = model.encode(main_statements, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(main_embeddings, f)\n",
    "    print(\"Created cache file for main embeddings!\")\n",
    "\n",
    "# ---------------------------\n",
    "# BATCH ENCODING IMPROVEMENT\n",
    "# ---------------------------\n",
    "print(\"Encoding compare statements in batch...\")\n",
    "compare_statements = df_compare['Statement'].tolist()\n",
    "compare_embeddings = model.encode(compare_statements, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "print(\"Computing similarity scores in a single pass...\")\n",
    "# This creates a similarity matrix of shape (len(df_compare), len(df_main))\n",
    "similarity_matrix = util.pytorch_cos_sim(compare_embeddings, main_embeddings)\n",
    "\n",
    "# Find the highest similarity score for each row in df_compare\n",
    "best_scores, best_idxs = similarity_matrix.max(dim=1)\n",
    "\n",
    "# Set a threshold if you want to discard matches below a certain score\n",
    "threshold = 0.1\n",
    "Result = []\n",
    "\n",
    "# Loop through each compare statement once, retrieving the best match\n",
    "for i in range(len(compare_statements)):\n",
    "    score = best_scores[i].item()\n",
    "    idx = best_idxs[i].item()\n",
    "    if score >= threshold:\n",
    "        best_document = df_main.iloc[idx]['Document']\n",
    "        best_statement = df_main.iloc[idx]['Statement']\n",
    "        folder_location = df_main.iloc[idx]['Folder location']\n",
    "        Result.append({\n",
    "            'Number': df_compare.iloc[i]['Number'],\n",
    "            'Statement': compare_statements[i],\n",
    "            'Matched Statement': best_statement,\n",
    "            'Matched Document Reference': best_document,\n",
    "            'Similarity Score': score,\n",
    "            'Folder location': folder_location\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "output_df = pd.DataFrame(Result)\n",
    "#print(output_df)\n",
    "\n",
    "# Write the output to Excel with XlsxWriter\n",
    "output_file = 'Excel_file/Result.xlsx'\n",
    "with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "    output_df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "\n",
    "    # Format the \"Similarity Score\" column (E) to display as a percentage\n",
    "    percentage_format = workbook.add_format({'num_format': '0.00%'})\n",
    "    worksheet.set_column('E:E', 16, percentage_format)\n",
    "\n",
    "    # Apply conditional formatting based on similarity score\n",
    "    num_rows = len(output_df)\n",
    "    cell_range = f'E2:E{num_rows + 1}'\n",
    "\n",
    "    red_format = workbook.add_format({'bg_color': '#FFC7CE', 'font_color': '#9C0006'})\n",
    "    orange_format = workbook.add_format({'bg_color': '#FFEB9C', 'font_color': '#9C6500'})\n",
    "    green_format = workbook.add_format({'bg_color': '#C6EFCE', 'font_color': '#006100'})\n",
    "\n",
    "    # < 80%: red\n",
    "    worksheet.conditional_format(cell_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '<',\n",
    "        'value': 0.8,\n",
    "        'format': red_format\n",
    "    })\n",
    "\n",
    "    # 80% - 95%: orange\n",
    "    worksheet.conditional_format(cell_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': 'between',\n",
    "        'minimum': 0.8,\n",
    "        'maximum': 0.95,\n",
    "        'format': orange_format\n",
    "    })\n",
    "\n",
    "    # >= 95%: green\n",
    "    worksheet.conditional_format(cell_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '>=',\n",
    "        'value': 0.95,\n",
    "        'format': green_format\n",
    "    })\n",
    "\n",
    "    # Adjust column widths\n",
    "    worksheet.set_column('A:A', 8)\n",
    "    worksheet.set_column('B:B', 50)\n",
    "    worksheet.set_column('C:C', 50)\n",
    "    worksheet.set_column('D:D', 60)\n",
    "    worksheet.set_column('F:F', 30)\n",
    "\n",
    "# End time and total time\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time: {total_time:.3f} seconds taken\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Model: 100%|██████████| 1/1 [00:03<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "Loaded cache file for main embeddings!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 35/35 [00:00<00:00, 54.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 3.965 seconds taken\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Old cold - unbatch processing in loop - slighty slower in redudant testing, faster initials\n",
    "# Import the required lib\n",
    "import pandas as pd # for data manipulation\n",
    "\n",
    "# Sentence Transformers enables the transformation of sentences into vector spaces\n",
    "from sentence_transformers import SentenceTransformer, util # util provides helper function for embeddings such as the function pytorch_cos_sim to compute cosine similarity\n",
    "from tqdm import tqdm # for progress bar\n",
    "import time # for total time\n",
    "import pickle # for caching main embeddings\n",
    "import os\n",
    "\n",
    "# Import the two excel file - input file and reference file\n",
    "df_main = pd.read_excel('Excel_file/Main.xlsx')\n",
    "df_compare = pd.read_excel('Excel_file/Compare.xlsx')\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Import thai compatible model\n",
    "print(\"Start loading model...\")\n",
    "with tqdm(total=1, desc=\"Loading Model\") as pbar:\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    pbar.update(1)\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Cache file for main embeddings\n",
    "cache_file = 'main_embeddings.pkl'\n",
    "\n",
    "# Use pickle to cache main file embeddings - load if already created, create if not\n",
    "# Note: When excel file is changed, the embeddings will need to be re-created, delete the cache file (main_embeddings.pkl)\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        main_embeddings = pickle.load(f)\n",
    "    print(\"Loaded cache file for main embeddings!\")\n",
    "else:\n",
    "    # Encode all statements from Main.xlsx as a single batch\n",
    "    print(\"Start embedding main statements...\")\n",
    "    main_statements = df_main['Statement'].tolist()\n",
    "    main_embeddings = model.encode(main_statements, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    # Cache the embeddings\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(main_embeddings, f)\n",
    "    \n",
    "    print(\"Created cache file for main embeddings!\")\n",
    "\n",
    "# Create similarity function - single batch variant\n",
    "def find_match(statement, main_df, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Finds the best match for 'statement' within 'main_df' using sentence-transformers semantic similarity.\n",
    "\n",
    "    Args:\n",
    "        statement (str): The statement from df_compare to match against df_main.\n",
    "        main_df (pd.DataFrame): DataFrame containing 'Statement' and 'Document' columns.\n",
    "        threshold (float): Minimum similarity score required to consider a match valid.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_document, best_statement, best_score)\n",
    "            - best_document: The matching 'Document' from df_main\n",
    "            - best_statement: The matched statement from df_main\n",
    "            - best_score: The highest cosine similarity score found\n",
    "    \"\"\"\n",
    "    # Encode the input statement once\n",
    "    embedding_input = model.encode(statement, convert_to_tensor=True)\n",
    "\n",
    "    # Compute similarity to all main embeddings at once (shape: (1, n_main))\n",
    "    similarity_scores = util.pytorch_cos_sim(embedding_input, main_embeddings)[0] # [0] extracts the first row from 2D tensor, this is similarity score for each statement in main_df\n",
    "    # Get the best score and its index\n",
    "    best_score, best_idx = similarity_scores.max(dim=0) # This will get the highest similarity score from the 1D tensor and its index, dim = 0 means row\n",
    "    best_score = best_score.item() # convert pytorch into float, Ex. 0.95\n",
    "    best_idx = best_idx.item() # convert pytorch into float, Ex. 3\n",
    "\n",
    "    # Threshold checking, return none if below threshold, which will skipped the append later\n",
    "    if best_score >= threshold:\n",
    "        # Retrieve the corresponding row from df_main with the best index from before\n",
    "        best_document = main_df.iloc[best_idx]['Document']  # Adjust column name if needed\n",
    "        best_statement = main_df.iloc[best_idx]['Statement']\n",
    "        folder_location = main_df.iloc[best_idx]['Folder location']\n",
    "        return best_document, best_statement, best_score, folder_location\n",
    "    else:\n",
    "        return None, None, best_score, None\n",
    "\n",
    "Result = []\n",
    "# Loop through each row, tqdm for progress bar\n",
    "for _, row in tqdm(df_compare.iterrows(), total=len(df_compare), desc=\"Processing rows\"):\n",
    "    # Use function to find the best match\n",
    "    document, statement, score, location = find_match(row['Statement'], df_main, threshold=0.1)\n",
    "\n",
    "    # If not none, then append to result\n",
    "    if document is not None:\n",
    "        Result.append({\n",
    "            'Number': row['Number'],\n",
    "            'Statement': row['Statement'],\n",
    "            'Matched Statement': statement,\n",
    "            'Matched Document Reference': document,\n",
    "            'Similarity Score': score,\n",
    "            'Folder location': location\n",
    "        })\n",
    "\n",
    "# Print the Dataframe result\n",
    "output_df = pd.DataFrame(Result)\n",
    "#print(output_df)\n",
    "\n",
    "# Use pandas to create an Excel file with XlsxWriter module with similarity coloring base on three conditions\n",
    "output_file = 'Excel_file/Result.xlsx'\n",
    "with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "    # write a dataframe into an excel file\n",
    "    output_df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "    # Get the workbook and worksheet object\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "    \n",
    "    # Determine the cell range for the Similarity Score column (row 2 to the last row)\n",
    "    num_rows = len(output_df)\n",
    "    cell_range = f'E2:E{num_rows + 1}'\n",
    "    \n",
    "    # Apply conditional formatting:\n",
    "    # Red for scores below 80% (< 0.8)\n",
    "    red_format = workbook.add_format({'bg_color': '#FFC7CE', 'font_color': '#9C0006'})\n",
    "    worksheet.conditional_format(cell_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '<',\n",
    "        'value': 0.8,\n",
    "        'format': red_format\n",
    "    })\n",
    "    \n",
    "    # Orange for scores between 80% and 95% (0.8 to 0.95)\n",
    "    orange_format = workbook.add_format({'bg_color': '#FFEB9C', 'font_color': '#9C6500'})\n",
    "    worksheet.conditional_format(cell_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': 'between',\n",
    "        'minimum': 0.8,\n",
    "        'maximum': 0.95,\n",
    "        'format': orange_format\n",
    "    })\n",
    "    \n",
    "    # Green for scores 95% and above (>= 0.95)\n",
    "    green_format = workbook.add_format({'bg_color': '#C6EFCE', 'font_color': '#006100'})\n",
    "    worksheet.conditional_format(cell_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '>=',\n",
    "        'value': 0.95,\n",
    "        'format': green_format\n",
    "    })\n",
    "\n",
    "    # Set column width for each column\n",
    "    worksheet.set_column('A:A', 8)\n",
    "    worksheet.set_column('B:B', 50)\n",
    "    worksheet.set_column('C:C', 50)\n",
    "    worksheet.set_column('D:D', 65)\n",
    "    worksheet.set_column('F:F', 30)\n",
    "    \n",
    "    # column E format is set to percentage instead, round to 2 decimal point\n",
    "    percentage_format = workbook.add_format({'num_format': '0.00%'})\n",
    "    worksheet.set_column('E:E', 14, percentage_format)\n",
    "\n",
    "\n",
    "# Get end time and total time taken\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time: {total_time:.3f} seconds taken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing output folder...\n",
      "unlinking file: output\\1.14.1.pdf\n",
      "unlinking file: output\\1.14.2.pdf\n",
      "\n",
      "\n",
      "Opening PDF: ODB_13.pdf ...\n",
      "Annotation content updated to: 2.1\n",
      "Annotation content updated to: 2.1\n",
      "Annotation content updated to: 2.1\n",
      "Opening PDF: ODB_14.pdf ...\n",
      "Annotation content updated to: 2.2\n",
      "Annotation content updated to: 2.2\n",
      "Opening PDF: ODB_15.pdf ...\n",
      "Annotation content updated to: 2.3\n",
      "Opening PDF: ODB_16.pdf ...\n",
      "Annotation content updated to: 2.4\n",
      "Annotation content updated to: 2.4\n",
      "Annotation content updated to: 2.4\n",
      "Annotation content updated to: 2.4\n",
      "Annotation content updated to: 2.4\n",
      "Annotation content updated to: 2.4\n",
      "Opening PDF: ODB_17.pdf ...\n",
      "Annotation content updated to: 2.5\n",
      "Opening PDF: ODB_18.pdf ...\n",
      "Annotation content updated to: 2.6\n"
     ]
    }
   ],
   "source": [
    "# testing PDF editor with python (pip install PyMuPDF)\n",
    "import fitz\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Clear output folder\n",
    "output_dir = 'output'\n",
    "if os.path.exists(output_dir):\n",
    "    print(\"Clearing output folder...\")\n",
    "    # Go over the files and subdirectories in the output folder\n",
    "    for filename in os.listdir(output_dir):\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # remove file or link\n",
    "                print(\"unlinking file: \" + file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # remove directory and its contents\n",
    "                print(\"removing directory: \" + file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "else:\n",
    "    # Create the output folder if it does not exist\n",
    "    print(\"Making output folder...\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"\\n\")\n",
    "# this will be from the complier.py file (extracted from excel input file)\n",
    "# Here's a simple Map instead, replace this with actual columns from either the result.xlsx or dataframe column to map, number will be from the result.xlsx\n",
    "pdf_statements = {\n",
    "    \"ODB_13\": \"2.1\",\n",
    "    \"ODB_14\": \"2.2\",\n",
    "    \"ODB_15\": \"2.3\",\n",
    "    \"ODB_16\": \"2.4\",\n",
    "    \"ODB_17\": \"2.5\",\n",
    "    \"ODB_18\": \"2.6\",\n",
    "}\n",
    "# loop over each PDF file\n",
    "for saved_statement, cur_statement in pdf_statements.items():\n",
    "    print(f\"Opening PDF: {saved_statement}.pdf ...\")\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(f'document/{saved_statement}.pdf')\n",
    "\n",
    "    # loop over all pages in PDF file\n",
    "    for page in doc:\n",
    "        # loop over all annotations\n",
    "        for annot in page.annots():\n",
    "            # check if annotation is a freetext annotation\n",
    "            if \"FreeText\" in annot.type:\n",
    "                # Update annotation text\n",
    "                annot.set_info(content=cur_statement, title=\"Oracle\")\n",
    "                annot.update()  # save annotation\n",
    "\n",
    "                print(f\"Annotation content updated to: {cur_statement}\")\n",
    "\n",
    "    # Save the PDF file\n",
    "    doc.save(f'output/{cur_statement}.pdf')\n",
    "    doc.close()  # Close the document when done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start embedding main statements...\n",
      "['เป็นระบบจัดการฐานข้อมูลเชิงสัมพันธ์ (Relational Database Management System)', 'เป็นระบบจัดการฐานข้อมูลเชิงสัมพันธ์ที่สนับสนุนการทำงานแบบออปเจ็กต์ (Object-Relational Database Management System)', 'เป็นระบบจัดการฐานข้อมูลเชิงสัมพันธ์ที่สนับสนุนการทำงานแบบออบเจ็กต์ (Object-Relational Database Management System) โดยสามารถทำงานกับข้อมูลต่าง ๆ ดังนี้ ', 'สามารถเลือกทำงานบนระบบปฏิบัติการ Windows หรือ ระบบปฏิบัติการ UNIX', 'สามารถทำงานได้บนระบบปฎิบัติการ Linux และ Windows', 'สามารถติดตั้งบนระบบปฏิบัติการ Linux และระบบปฏิบัติการ Windows ได้เป็นอย่างน้อย', 'เป็นฐานข้อมูลที่มีระบบ Lock ข้อมูลในระดับ Row Level Locking จริง ๆ ซึ่ง Database Engine กระทำได้เอง โดยต้องไม่มีการเขียนโปรแกรมเพิ่มเติม', 'สามารถใช้งาน Lock ข้อมูลในระดับแถว (Row Level Locking) ได้อัตโนมัติ โดยไม่ต้องมีการพัฒนาโปรแกรมเพิ่มเติม', 'มีคุณสมบัติในการทำ Multi-Version Read Consistency โดยไม่มีการอ่านข้อมูลแบบ Dirty Reads ทั้งนี้เพื่อความถูกต้องของข้อมูลที่จะถูกนำไปใช้ ที่ซึ่งผู้เป็น Readers และ Writers ของข้อมูลจะต้องไม่ Block ซึ่งกันและกัน (ผู้อ่านข้อมูลไม่ Block ผู้เขียนข้อมูล และผู้เขียนข้อมูลไม่ Block ผู้อ่านข้อมูล)', 'สามารถให้ความสอดคล้องในการอ่านข้อมูล (Multi-Version Read Consistency) โดยไม่สร้างผลลัพธ์ที่ไม่ถูกต้อง (Dirty Reads) และไม่บล็อกการอ่านและเขียนข้อมูลของผู้ใช้อื่น', 'มีคุณสมบัติในการทำ Multi-Version Read Consistency โดยไม่มีการอ่านข้อมูลแบบ Dirty Reads ทั้งนี้เพื่อความถูกต้องของข้อมูลที่จะถูกนำไปใช้ โดย Readers และ Writers ของข้อมูลจะต้องไม่ block ซึ่งกันและกัน ', 'มีคุณสมบัติในการทำ Multi-Version Read Consistency โดยไม่มีการอ่านข้อมูลแบบ Dirty Reads ทั้งนี้เพื่อความถูกต้องของข้อมูลที่จะถูกนำไปใช้ ที่ซึ่งผู้เป็น Readers และ Writers ของข้อมูลจะต้องไม่ block ซึ่งกันและกัน (ผู้อ่านข้อมูลไม่ block ผู้เขียนข้อมูล และผู้เขียนข้อมูลไม่ block ผู้อ่านข้อมูล) ดังนั้นเมื่อมี transaction ที่ทำการเปลี่ยนแปลงข้อมูลแต่ยังไม่มีการ commit หรือ rollback ผู้ใช้งานอื่นจะต้องสามารถอ่านข้อมูลใน row นั้นได้ โดยเห็นข้อมูลของชุดก่อนที่จะมีการเปลี่ยนแปลงเนื่องจากข้อมูลดังกล่าวยังไม่มีการยืนยันการเปลี่ยนแปลงที่เป็นจริง', 'สามารถทำการเก็บข้อมูลและแสดงผลได้ทั้งภาษาไทยและภาษาอังกฤษ', 'มีการทำงานแบบ Machine Learning เพื่อช่วยเพิ่มประสิทธิภาพในการทำงานของระบบฐานข้อมูล', 'มีการทำงานแบบ Query Optimization และสามารถทำงานร่วมกับ Machine Learning เพื่อช่วยเพิ่มประสิทธิภาพในการทำงานของระบบฐานข้อมูล', 'สามารถรองรับการจัดเก็บข้อมูลในรูปแบบ JSON โดยสามารถค้นหาข้อมูลที่ถูกจัดเก็บในรูปแบบ JSON ได้', 'สามารถทำงานในรูปแบบระบบฐานข้อมูลแบบ Graph ได้', 'มีเครื่องมือรองรับในการจัดการระบบไฟล์สำหรับไฟล์ฐานข้อมูลโดยเฉพาะ โดยมีคุณสมบัติดังนี้', 'รองรับการช่วยกระจาย I/O ไปยังดิสก์ข้อมูลต่าง ๆ เพื่อเพิ่มประสิทธิภาพในการทำงาน', 'ช่วยกระจาย I/O ไปยังดิสก์ข้อมูลต่าง ๆ เพื่อเพิ่มประสิทธิภาพในการทำงาน และช่วยลดเรื่อง hotspots ของดิสก์ในฐานข้อมูล', 'รองรับการเพิ่มหรือลดจำนวน disk ได้โดยไม่กระทบกับระบบ', 'สามารถทำการเพิ่มดิสก์หรือลบดิสก์ออกจากฐานข้อมูลด้วยคำสั่ง SQL โดยไม่ต้องหยุดการทำงานของฐานข้อมูล และสามารถจัดการ Disk groups ด้วยเครื่องมือที่เป็น GUI ได้', 'รองรับการจัดเรียงการกระจายของข้อมูลใหม่ (Redistribution หรือ Rebalancing) ในกรณีที่มีการเพิ่มดิสก์หรือลบดิสก์ออกจากฐานข้อมูล', 'สามารถทำการจัดเรียงการกระจายของข้อมูลใหม่ (Redistribution หรือ Rebalancing) ในกรณีที่มีการเพิ่มดิสก์หรือลบดิสก์ออกจากฐานข้อมูล โดยการจัดเรียงดังกล่าวต้องทำงานในรูปแบบ background เพื่อให้กระทบกับประสิทธิภาพการทำงานของฐานข้อมูลน้อยที่สุด', 'รองรับการ Mirror Resync ข้อมูลระหว่าง Disk กรณีที่ Disk failed หรือไม่สามารถทำงานได้', 'สามารถทำการ Mirror Resync ข้อมูลระหว่าง Disk กรณีที่ Disk บางตัวมีการ Offline หรือไม่สามารถทำงานได้ โดยจะ Resync เฉพาะข้อมูลที่มีการเปลี่ยนแปลงระหว่าง Offline เท่านั้นเพื่อลดระยะเวลาในการทำงาน', 'รองรับการทำงานในลักษณะ Cluster (Active/Active)', 'มาพร้อมกับเครื่องมือในการสร้าง Web Application อย่างง่าย (Low-Code) โดยเครื่องมือนี้ต้องสนับสนุนการทำงานแบบ Web services และมีความสามารถในการทำ Drag and Drop สร้าง Chart ในรูปแบบต่าง ๆ ได้', 'สามารถทำงานแบบ Multi-Tenant ได้ไม่น้อยกว่า ๓ Tenant', 'ต้องสนับสนุน เน็ตเวิร์คโปรโตคอลแบบ TCP/IP เป็นอย่างน้อย', 'มีเครื่องมือที่ช่วยในการสร้างแอปพลิเคชันเว็บโดยใช้แนวคิด Low-Code ซึ่งสามารถสร้าง Web services และสร้างกราฟในรูปแบบ Drag and Drop ได้อย่างง่ายดาย', 'สามารถเลือกทำการ roll back กลับไป ณ ช่วงเวลาที่กำหนดได้ (point-in-time data recovery)', 'ระบบต้องมีความสามารถในการสร้าง Blockchain table เพื่อจัดเก็บข้อมูลที่มีความสำคัญซึ่งไม่สามารถแก้ไขเปลี่ยนแปลงได้ (insert only table) พร้อมกับ hash value ที่จะมีความสัมพันธ์กับรายการ (row) ก่อนหน้า โดยต้องสามารถตรวจสอบความถูกต้องของข้อมูล ในแต่ละรายการ (row) จาก hash value ที่ไม่ผ่านการแก้ไขได้', 'สามารถทำการเปลี่ยนแปลง Database Password ของ Application User ได้ โดยไม่ก่อให้เกิด Downtime ทำให้การทำงานของ Application ไม่ถูกกระทบ ขณะทำการเปลี่ยน Password', 'รองรับการสำเนาข้อมูล (Mirroring) ในระดับ file โดยรองรับการสำเนาทั้งใน รูปแบบสองสำเนา และสามสำเนา (2-Way Mirroring และ 3- Way Mirroring)', 'ข้อมูลเชิงสัมพันธ์ (Relational)', 'ข้อมูล Sharded', 'ข้อมูลเอกสาร (Document Store) เช่น JSON, XML, Text ', 'ข้อมูลเชิงแผนที่ (Spatial)', 'ข้อมูล Graph และ Triple Store', 'สามารถทำการสำรองและกู้ข้อมูล (Database Backup/Restore) ได้ในลักษณะต่อไปนี้', 'Full Database Backup', 'Incremental Backup', 'Online Backup', 'สามารถสนับสนุนการทำงานแบบ Parallel โดยมีคุณสมบัติดังนี้', 'Parallel Statistics Gathering', 'Parallel Index Build/Scans', 'Parallel Data Pump Export/Import', 'Parallel Query/DML', 'Parallel Backup and Recovery', 'มีเครื่องมือในการสร้าง Web Application ในลักษณะ Low Coding ที่สามารถสร้างหน้าจอ ', 'Form สำหรับเพิ่ม ปรับปรุง และลบข้อมูล', 'สามารถสร้างหน้าจอจัดการข้อมูลในลักษณะ Grid ', 'สามารถเชื่อมโยงข้อมูลผ่าน REST API', 'สามารถสร้าง Chart ต่างๆ ที่รองการแสดงผ่าน HTML5 เป็น Responsive ', 'เป็นระบบ Data Mining ที่ embedded อยู่ในระบบ Data Service Platform เพื่อลดการเคลื่อนย้ายข้อมูลออกจากฐานข้อมูล', 'สามารถพัฒนา โดยใช้ SQL, Stored Procedures ,PL/SQL Server และ Trigger ได้', 'สามารถรองรับลักษณะงานที่เป็น Analytic workload โดยมีคุณสมบัติดังนี้', 'สามารถจัดเก็บข้อมูลในรูปแบบ Columnar data store', 'สามารถสร้างตาราง (Table) ในรูปแบบ Row-based และ Column-based ในฐานข้อมูลเดียวกัน', 'สามารถประมวลผล Query ได้อย่างมีประสิทธิภาพโดยไม่ต้องสร้าง Indexes', 'สามารถประมวลผลในรูปแบบที่เป็น massively parallel processing (MPP) หรือparallel execution', 'สามารถทำงานในรูปแบบ Cluster Database แบบ Active-Active หรือ Active-Passive ได้ และ ต้องสามารถทำงานแบบ Multi-Tenant ได้โดยต้องมีลิขสิทธิ์การทำงานไม่ต่ำกว่า 3 tenants', 'สามารถทำงานได้บนระบบปฏิบัติการ UNIX, Linux และ Windows ได้เป็นอย่างน้อย', 'สามารถเชื่อมต่อฐานข้อมูลด้วยเน็ตเวิร์คโพรโทคอล TCP/IP , HTTP , WebDAV และ FTP ได้เป็นอย่างน้อย', 'สามารถเชื่อมต่อฐานข้อมูลด้วยเน็ตเวิร์คโพรโทคอล TCP/IP , HTTP , WebDAV และ FTP ได้เป็นอย่างน้อย', 'สามารถเชื่อมต่อฐานข้อมูลด้วยมาตรฐาน', 'Open Database Connectivity (ODBC)', 'Java Database Connectivity (JDBC)', 'Net, PHP, Python, C, C++', 'สามารถรองรับการทำงานกับข้อมูลรูปแบบ Character , Variable Character , Numeric , Date , BLOB , XML และ JSON ได้เป็นอย่างน้อย', 'สนับสนุนการทำ Index, Referential Integrity, Unique Constraints', 'สนับสนุนการทำ View, Stored Procedure, Function และ Trigger ']\n"
     ]
    }
   ],
   "source": [
    "# test why cannot embeded\n",
    "import pandas as pd\n",
    "df_main = pd.read_excel('Excel_file/ODB_Mapped_PDF.xlsx')\n",
    "print(\"Start embedding main statements...\")\n",
    "main_statements = df_main['Statement'].tolist()\n",
    "#main_embeddings = model.encode(main_statements, convert_to_tensor=True, show_progress_bar=True)\n",
    "print(main_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.1</td>\n",
       "      <td>\\tเป็นระบบจัดเก็บฐานข้อมูลเชิงสัมพันธ์ที่สนับส...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.2</td>\n",
       "      <td>สามารถทำงานได้บนระบบปฏิบัติการ ได้แก่ Linux แล...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.3</td>\n",
       "      <td>สามารถเชื่อมต่อฐานข้อมูลด้วยเน็ตเวิร์คโพรโทคอล...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.4</td>\n",
       "      <td>สามารถเชื่อมต่อฐานข้อมูลด้วยมาตรฐาน Open Datab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.5</td>\n",
       "      <td>สามารถรองรับการทำงานกับข้อมูลรูปแบบ Character ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.6</td>\n",
       "      <td>สนับสนุนการทำ Index, Referential Integrity, Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.7</td>\n",
       "      <td>สนับสนุนการทำ View, Stored Procedure, Function...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.8</td>\n",
       "      <td>มีระบบช่วยเหลือในการสืบค้นข้อมูล (Query Optimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.9</td>\n",
       "      <td>มีคุณสมบัติในการทำ Multi-Version Read Consiste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.10</td>\n",
       "      <td>สามารถสำรองและกู้คืนฐานข้อมูล (Database Backup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.11</td>\n",
       "      <td>สามารถทำการเก็บข้อมูลและแสดงผลได้ทั้งภาษาไทยแล...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.12</td>\n",
       "      <td>มีเครื่องมือในการสร้าง Web Application อย่างง่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.13</td>\n",
       "      <td>รองรับทำการสร้าง Machine Learning Model โดยมี ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.14</td>\n",
       "      <td>รองรับทำการสร้าง Database แบบ Multitenant โดยม...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.15</td>\n",
       "      <td>รองรับการทำ High Availability ทำงานได้แบบ Acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.1</td>\n",
       "      <td>สามารถจำกัดสิทธิ์ของผู้ดูแลฐานข้อมูล (Database...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9.2</td>\n",
       "      <td>สามารถกำหนดช่วงเวลาในการทำงานของ DBA และผู้ใช้...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9.3</td>\n",
       "      <td>สามารถจำกัดสิทธิ์ในการเปลี่ยนสิทธ์ของผู้ดูแลระ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9.4</td>\n",
       "      <td>สามารถเชื่อมต่อกับระบบ Monitoring ต่างๆ ได้ เช...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.5</td>\n",
       "      <td>สามารถทำการเข้ารหัสข้อมูลในระดับตาราง (Tables)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number                                          Statement\n",
       "0     3.1  \\tเป็นระบบจัดเก็บฐานข้อมูลเชิงสัมพันธ์ที่สนับส...\n",
       "1     3.2  สามารถทำงานได้บนระบบปฏิบัติการ ได้แก่ Linux แล...\n",
       "2     3.3  สามารถเชื่อมต่อฐานข้อมูลด้วยเน็ตเวิร์คโพรโทคอล...\n",
       "3     3.4  สามารถเชื่อมต่อฐานข้อมูลด้วยมาตรฐาน Open Datab...\n",
       "4     3.5  สามารถรองรับการทำงานกับข้อมูลรูปแบบ Character ...\n",
       "5     3.6  สนับสนุนการทำ Index, Referential Integrity, Un...\n",
       "6     3.7  สนับสนุนการทำ View, Stored Procedure, Function...\n",
       "7     3.8  มีระบบช่วยเหลือในการสืบค้นข้อมูล (Query Optimi...\n",
       "8     3.9  มีคุณสมบัติในการทำ Multi-Version Read Consiste...\n",
       "9    3.10  สามารถสำรองและกู้คืนฐานข้อมูล (Database Backup...\n",
       "10   3.11  สามารถทำการเก็บข้อมูลและแสดงผลได้ทั้งภาษาไทยแล...\n",
       "11   3.12  มีเครื่องมือในการสร้าง Web Application อย่างง่...\n",
       "12   3.13  รองรับทำการสร้าง Machine Learning Model โดยมี ...\n",
       "13   3.14  รองรับทำการสร้าง Database แบบ Multitenant โดยม...\n",
       "14   3.15  รองรับการทำ High Availability ทำงานได้แบบ Acti...\n",
       "15    9.1  สามารถจำกัดสิทธิ์ของผู้ดูแลฐานข้อมูล (Database...\n",
       "16    9.2  สามารถกำหนดช่วงเวลาในการทำงานของ DBA และผู้ใช้...\n",
       "17    9.3  สามารถจำกัดสิทธิ์ในการเปลี่ยนสิทธ์ของผู้ดูแลระ...\n",
       "18    9.4  สามารถเชื่อมต่อกับระบบ Monitoring ต่างๆ ได้ เช...\n",
       "19    9.5  สามารถทำการเข้ารหัสข้อมูลในระดับตาราง (Tables)..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dealing with zeros -> make sure number col is in \"text\" format and trailing zero is correct\n",
    "import pandas as pd\n",
    "df_main = pd.read_excel(\n",
    "    'Excel_file/Compare.xlsx',\n",
    "    sheet_name=\"ODB\",\n",
    "    header=1,\n",
    "    dtype={'Number': str}  # Force 'Number' column to be read as text\n",
    ")\n",
    "df_main.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
