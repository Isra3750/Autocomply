{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy is like the Swiss Army knife of NLP, while Transformers is more akin to a sledge hammer.\n",
    "\n",
    "SpaCy is fast and lightweight. Transformers (ie. Sentence transformer) let’s you use state of the art stuff, but the trade off is usually in terms of slower runtime at inference and larger memory usage.\n",
    "\n",
    "Another important distinction is that SpaCy has tools for more linguistics-focused tasks, such as dependency parsing, and annotations. While transformers has tools for tasks that span beyond just NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# the problem with fuzz is that it does not capture sematic meaning -> good ratio can be very bad since wording is key in TORs\n",
    "#str1 = 'สามารถติดตั้งบนระบบปฏิบัติการต่าง ๆ เช่น Windows Server, Linux, Unix ได้เป็นอย่างน้อย'\n",
    "#str2 = 'สามารถเลือกทำงานบนระบบปฏิบัติการ Windows หรือ ระบบปฏิบัติการ UNIX'\n",
    "\n",
    "#str1 = 'สามารถใช้งาน Lock ข้อมูลในระดับแถว (Row Level Locking) ได้อัตโนมัติ โดยไม่ต้องมีการพัฒนาโปรแกรมเพิ่มเติม'\n",
    "#str2 = 'เป็นฐานข้อมูลที่มีระบบ Lock ข้อมูลในระดับ Row Level Locking จริง ๆ ซึ่ง Database Engine กระทำได้เอง โดยต้องไม่มีการเขียนโปรแกรมเพิ่มเติม'\n",
    "\n",
    "str1 = 'Im very happy right now, so thats the best right'\n",
    "str2 = 'Im very sad right now, so thats the best right'\n",
    "\n",
    "display(fuzz.token_sort_ratio(str1, str2)) # token based -> order does not matter as much as long as words are the same\n",
    "display(fuzz.ratio(str1, str2)) # Order matters -> whitespace also effect the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzz does not give good result for similar text - if not excat match. Compare this to sentencetransformer below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.7313247919082642\n"
     ]
    }
   ],
   "source": [
    "# test using sentence models -> pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# load a pre-trained model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Define the two sentences\n",
    "sentence1 = 'Im very happy right now, so thats the best right'\n",
    "sentence2 = 'Im very sad right now, so thats the best right'\n",
    "\n",
    "# Generate embeddings for each sentence\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity between the embeddings\n",
    "cosine_score = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(\"Cosine similarity:\", cosine_score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.8878823518753052\n"
     ]
    }
   ],
   "source": [
    "# test using sentence models -> pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define the two sentences\n",
    "sentence1 = 'I want to really eat some ice cream at the store'\n",
    "sentence2 = 'I want to really not eat some ice cream at the storesssssssssss'\n",
    "\n",
    "# Generate embeddings for each sentence\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity between the embeddings\n",
    "cosine_score = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(\"Cosine similarity:\", cosine_score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_trf'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create spacy nlp object\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# load en_core_web_md (small model), en_core_web_lg (large model), en_core_web_trf (largest)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# pip uninstall en-core-web-lg\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#nlp = spacy.load(\"en_core_web_lg\")\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_trf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(nlp\u001b[38;5;241m.\u001b[39mmeta)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_trf'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Create spacy nlp object\n",
    "# load en_core_web_md (small model), en_core_web_lg (large model), en_core_web_trf (largest)\n",
    "# pip uninstall en-core-web-lg\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "print(nlp.meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statement accuracy rate, compare between sentence transformer vs spacy vs fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 11/11 [00:01<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([341, 384])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # for data manipulation\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Import the two excel file - input file and reference file\n",
    "df_main = pd.read_excel('Excel_file/Main.xlsx')\n",
    "df_compare = pd.read_excel('Excel_file/Compare.xlsx')\n",
    "\n",
    "# Import thai compatible model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Encode all statements from Main.xlsx as a single batch\n",
    "main_statements = df_main['Statement'].tolist()\n",
    "main_embeddings = model.encode(main_statements, convert_to_tensor=True, show_progress_bar=True)\n",
    "print(type(main_embeddings))\n",
    "main_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # for caching main embeddings\n",
    "\n",
    "# testing pickle, pk1 is pickle file, can be any file type really but pk1 just to demonstrate\n",
    "student_names = ['Kay','Bob','Elena','Jane','Kyle']\n",
    "with open('student_file.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(student_names, f) # serialize the list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kay', 'Bob', 'Elena', 'Jane', 'Kyle']\n"
     ]
    }
   ],
   "source": [
    "with open('student_file.pkl', 'rb') as f:  # open a text file\n",
    "    list_name = pickle.load(f) # deserialize the list\n",
    "f.close()\n",
    "print(list_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TOR comply number                               TOR comply statement\n",
      "5                4.2  สามารถเลือกทำงานบนระบบปฏิบัติการ Windows หรือ ...\n",
      "6                4.3  เป็นฐานข้อมูลที่มีระบบ Lock ข้อมูลในระดับ Row ...\n",
      "7                4.4  มีคุณสมบัติในการทำ Multi-Version Read Consiste...\n",
      "8                4.5  สามารถทำการเก็บข้อมูลและแสดงผลได้ทั้งภาษาไทยแล...\n",
      "9                4.6  มีการทำงานแบบ Machine Learning เพื่อช่วยเพิ่มป...\n",
      "10               4.7  มีการทำงานแบบ Query Optimization และสามารถทำงา...\n",
      "11               4.8  สามารถรองรับการจัดเก็บข้อมูลในรูปแบบ JSON โดยส...\n",
      "12               4.9       สามารถทำงานในรูปแบบระบบฐานข้อมูลแบบ Graph ได\n",
      "13               4.1  มีเครื่องมือรองรับในการจัดการระบบไฟล์สำหรับไฟล...\n",
      "14               NaN  4.10.1 รองรับการช่วยกระจาย I/O ไปยังดิสก์ข้อมู...\n",
      "15               NaN  4.10.2 รองรับการเพิ่มหรือลดจำนวน disk ได้โดยไม...\n",
      "16               NaN  4.10.3 รองรับการจัดเรียงการกระจายของข้อมูลใหม่...\n",
      "17               NaN  4.10.4 รองรับการ Mirror Resync ข้อมูลระหว่าง D...\n",
      "18              4.11     รองรับการทำงานในลักษณะ Cluster (Active/Active)\n",
      "19              4.12  มาพร้อมกับเครื่องมือในการสร้าง Web Application...\n",
      "20              4.13  สามารถทำงานแบบ Multi-Tenant ได้ไม่น้อยกว่า 3Te...\n",
      "21              4.14  ต้องสนับสนุน เน็ตเวิร์คโปรโตคอลแบบ TCP/IP เป็น...\n",
      "22              4.15  มีลิขสิทธิ์ใช้งานถูกต้องตามกฎหมายแบบไม่จำกัดจำ...\n"
     ]
    }
   ],
   "source": [
    "# selecting excel test\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def excel_cell_to_indices(cell_str):\n",
    "    \"\"\"\n",
    "    Converts an Excel cell address (e.g., \"A5\") to zero-based (row, column) indices.\n",
    "    \"\"\"\n",
    "    match = re.match(r\"([A-Za-z]+)([0-9]+)\", cell_str)\n",
    "    if not match:\n",
    "        raise ValueError(\"Invalid cell format: \" + cell_str)\n",
    "    col_str, row_str = match.groups()\n",
    "    # Convert letters to a zero-based column index:\n",
    "    col_idx = 0\n",
    "    for char in col_str.upper():\n",
    "        col_idx = col_idx * 26 + (ord(char) - ord('A') + 1)\n",
    "    col_idx -= 1  # adjust to zero-based index\n",
    "    row_idx = int(row_str) - 1  # adjust to zero-based index\n",
    "    return row_idx, col_idx\n",
    "\n",
    "def slice_excel_by_cells(df, num_start, num_end, stmt_start, stmt_end):\n",
    "    \"\"\"\n",
    "    Extracts two series from the DataFrame based on provided Excel cell ranges.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame read from the Excel file.\n",
    "        num_start (str): Starting cell for TOR comply numbers (e.g., \"A5\").\n",
    "        num_end (str): Ending cell for TOR comply numbers (e.g., \"A23\").\n",
    "        stmt_start (str): Starting cell for TOR comply statements (e.g., \"B5\").\n",
    "        stmt_end (str): Ending cell for TOR comply statements (e.g., \"B23\").\n",
    "    \n",
    "    Returns:\n",
    "        (pd.Series, pd.Series): Two series, one for numbers and one for statements.\n",
    "    \"\"\"\n",
    "    num_start_row, num_start_col = excel_cell_to_indices(num_start)\n",
    "    num_end_row, _ = excel_cell_to_indices(num_end)  # Column should be same as start for numbers\n",
    "    stmt_start_row, stmt_start_col = excel_cell_to_indices(stmt_start)\n",
    "    stmt_end_row, _ = excel_cell_to_indices(stmt_end)  # Column should be same as start for statements\n",
    "    \n",
    "    # Slicing includes the ending row so add 1 (pandas slicing is end-exclusive)\n",
    "    numbers = df.iloc[num_start_row:num_end_row+1, num_start_col]\n",
    "    statements = df.iloc[stmt_start_row:stmt_end_row+1, stmt_start_col]\n",
    "    return numbers, statements\n",
    "\n",
    "# Example usage:\n",
    "# Read the Excel file (adjust header settings if needed)\n",
    "df = pd.read_excel(\"Excel_file/Unformat_test.xlsx\", header=None)\n",
    "\n",
    "# Dynamically select ranges using Excel cell notation.\n",
    "tor_numbers, tor_statements = slice_excel_by_cells(df, \"A6\", \"A23\", \"B6\", \"B23\")\n",
    "\n",
    "# Combine into a new DataFrame with proper column names\n",
    "result_df = pd.DataFrame({\n",
    "    \"TOR comply number\": tor_numbers,\n",
    "    \"TOR comply statement\": tor_statements\n",
    "})\n",
    "\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Model: 100%|██████████| 1/1 [00:03<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "Loaded cache file for main embeddings!\n",
      "Encoding compare statements in batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarity scores in a single pass...\n",
      "Total time: 3.595 seconds taken\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# single batch code - LLM generated\n",
    "import pandas as pd  # for data manipulation\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load the two Excel files\n",
    "df_main = pd.read_excel('Excel_file/Main.xlsx')\n",
    "df_compare = pd.read_excel('Excel_file/Compare.xlsx')\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "print(\"Start loading model...\")\n",
    "with tqdm(total=1, desc=\"Loading Model\") as pbar:\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    pbar.update(1)\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Cache file for main embeddings\n",
    "cache_file = 'main_embeddings.pkl'\n",
    "\n",
    "# Either load cached embeddings or create them if they don't exist\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        main_embeddings = pickle.load(f)\n",
    "    print(\"Loaded cache file for main embeddings!\")\n",
    "else:\n",
    "    print(\"Start embedding main statements...\")\n",
    "    main_statements = df_main['Statement'].tolist()\n",
    "    main_embeddings = model.encode(main_statements, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(main_embeddings, f)\n",
    "    print(\"Created cache file for main embeddings!\")\n",
    "\n",
    "# ---------------------------\n",
    "# BATCH ENCODING IMPROVEMENT\n",
    "# ---------------------------\n",
    "print(\"Encoding compare statements in batch...\")\n",
    "compare_statements = df_compare['Statement'].tolist()\n",
    "compare_embeddings = model.encode(compare_statements, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "print(\"Computing similarity scores in a single pass...\")\n",
    "# This creates a similarity matrix of shape (len(df_compare), len(df_main))\n",
    "similarity_matrix = util.pytorch_cos_sim(compare_embeddings, main_embeddings)\n",
    "\n",
    "# Find the highest similarity score for each row in df_compare\n",
    "best_scores, best_idxs = similarity_matrix.max(dim=1)\n",
    "\n",
    "# Set a threshold if you want to discard matches below a certain score\n",
    "threshold = 0.1\n",
    "Result = []\n",
    "\n",
    "# Loop through each compare statement once, retrieving the best match\n",
    "for i in range(len(compare_statements)):\n",
    "    score = best_scores[i].item()\n",
    "    idx = best_idxs[i].item()\n",
    "    if score >= threshold:\n",
    "        best_document = df_main.iloc[idx]['Document']\n",
    "        best_statement = df_main.iloc[idx]['Statement']\n",
    "        folder_location = df_main.iloc[idx]['Folder location']\n",
    "        Result.append({\n",
    "            'Number': df_compare.iloc[i]['Number'],\n",
    "            'Statement': compare_statements[i],\n",
    "            'Matched Statement': best_statement,\n",
    "            'Matched Document Reference': best_document,\n",
    "            'Similarity Score': score,\n",
    "            'Folder location': folder_location\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "output_df = pd.DataFrame(Result)\n",
    "#print(output_df)\n",
    "\n",
    "# Write the output to Excel with XlsxWriter\n",
    "output_file = 'Excel_file/Result.xlsx'\n",
    "with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "    output_df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "\n",
    "    # Format the \"Similarity Score\" column (E) to display as a percentage\n",
    "    percentage_format = workbook.add_format({'num_format': '0.00%'})\n",
    "    worksheet.set_column('E:E', 16, percentage_format)\n",
    "\n",
    "    # Apply conditional formatting based on similarity score\n",
    "    num_rows = len(output_df)\n",
    "    cell_range = f'E2:E{num_rows + 1}'\n",
    "\n",
    "    red_format = workbook.add_format({'bg_color': '#FFC7CE', 'font_color': '#9C0006'})\n",
    "    orange_format = workbook.add_format({'bg_color': '#FFEB9C', 'font_color': '#9C6500'})\n",
    "    green_format = workbook.add_format({'bg_color': '#C6EFCE', 'font_color': '#006100'})\n",
    "\n",
    "    # < 80%: red\n",
    "    worksheet.conditional_format(cell_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '<',\n",
    "        'value': 0.8,\n",
    "        'format': red_format\n",
    "    })\n",
    "\n",
    "    # 80% - 95%: orange\n",
    "    worksheet.conditional_format(cell_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': 'between',\n",
    "        'minimum': 0.8,\n",
    "        'maximum': 0.95,\n",
    "        'format': orange_format\n",
    "    })\n",
    "\n",
    "    # >= 95%: green\n",
    "    worksheet.conditional_format(cell_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '>=',\n",
    "        'value': 0.95,\n",
    "        'format': green_format\n",
    "    })\n",
    "\n",
    "    # Adjust column widths\n",
    "    worksheet.set_column('A:A', 8)\n",
    "    worksheet.set_column('B:B', 50)\n",
    "    worksheet.set_column('C:C', 50)\n",
    "    worksheet.set_column('D:D', 60)\n",
    "    worksheet.set_column('F:F', 30)\n",
    "\n",
    "# End time and total time\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time: {total_time:.3f} seconds taken\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Model: 100%|██████████| 1/1 [00:03<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "Loaded cache file for main embeddings!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 35/35 [00:00<00:00, 54.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 3.965 seconds taken\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Old cold - unbatch processing in loop - slighty slower in redudant testing, faster initials\n",
    "# Import the required lib\n",
    "import pandas as pd # for data manipulation\n",
    "\n",
    "# Sentence Transformers enables the transformation of sentences into vector spaces\n",
    "from sentence_transformers import SentenceTransformer, util # util provides helper function for embeddings such as the function pytorch_cos_sim to compute cosine similarity\n",
    "from tqdm import tqdm # for progress bar\n",
    "import time # for total time\n",
    "import pickle # for caching main embeddings\n",
    "import os\n",
    "\n",
    "# Import the two excel file - input file and reference file\n",
    "df_main = pd.read_excel('Excel_file/Main.xlsx')\n",
    "df_compare = pd.read_excel('Excel_file/Compare.xlsx')\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Import thai compatible model\n",
    "print(\"Start loading model...\")\n",
    "with tqdm(total=1, desc=\"Loading Model\") as pbar:\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    pbar.update(1)\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Cache file for main embeddings\n",
    "cache_file = 'main_embeddings.pkl'\n",
    "\n",
    "# Use pickle to cache main file embeddings - load if already created, create if not\n",
    "# Note: When excel file is changed, the embeddings will need to be re-created, delete the cache file (main_embeddings.pkl)\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        main_embeddings = pickle.load(f)\n",
    "    print(\"Loaded cache file for main embeddings!\")\n",
    "else:\n",
    "    # Encode all statements from Main.xlsx as a single batch\n",
    "    print(\"Start embedding main statements...\")\n",
    "    main_statements = df_main['Statement'].tolist()\n",
    "    main_embeddings = model.encode(main_statements, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    # Cache the embeddings\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(main_embeddings, f)\n",
    "    \n",
    "    print(\"Created cache file for main embeddings!\")\n",
    "\n",
    "# Create similarity function - single batch variant\n",
    "def find_match(statement, main_df, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Finds the best match for 'statement' within 'main_df' using sentence-transformers semantic similarity.\n",
    "\n",
    "    Args:\n",
    "        statement (str): The statement from df_compare to match against df_main.\n",
    "        main_df (pd.DataFrame): DataFrame containing 'Statement' and 'Document' columns.\n",
    "        threshold (float): Minimum similarity score required to consider a match valid.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_document, best_statement, best_score)\n",
    "            - best_document: The matching 'Document' from df_main\n",
    "            - best_statement: The matched statement from df_main\n",
    "            - best_score: The highest cosine similarity score found\n",
    "    \"\"\"\n",
    "    # Encode the input statement once\n",
    "    embedding_input = model.encode(statement, convert_to_tensor=True)\n",
    "\n",
    "    # Compute similarity to all main embeddings at once (shape: (1, n_main))\n",
    "    similarity_scores = util.pytorch_cos_sim(embedding_input, main_embeddings)[0] # [0] extracts the first row from 2D tensor, this is similarity score for each statement in main_df\n",
    "    # Get the best score and its index\n",
    "    best_score, best_idx = similarity_scores.max(dim=0) # This will get the highest similarity score from the 1D tensor and its index, dim = 0 means row\n",
    "    best_score = best_score.item() # convert pytorch into float, Ex. 0.95\n",
    "    best_idx = best_idx.item() # convert pytorch into float, Ex. 3\n",
    "\n",
    "    # Threshold checking, return none if below threshold, which will skipped the append later\n",
    "    if best_score >= threshold:\n",
    "        # Retrieve the corresponding row from df_main with the best index from before\n",
    "        best_document = main_df.iloc[best_idx]['Document']  # Adjust column name if needed\n",
    "        best_statement = main_df.iloc[best_idx]['Statement']\n",
    "        folder_location = main_df.iloc[best_idx]['Folder location']\n",
    "        return best_document, best_statement, best_score, folder_location\n",
    "    else:\n",
    "        return None, None, best_score, None\n",
    "\n",
    "Result = []\n",
    "# Loop through each row, tqdm for progress bar\n",
    "for _, row in tqdm(df_compare.iterrows(), total=len(df_compare), desc=\"Processing rows\"):\n",
    "    # Use function to find the best match\n",
    "    document, statement, score, location = find_match(row['Statement'], df_main, threshold=0.1)\n",
    "\n",
    "    # If not none, then append to result\n",
    "    if document is not None:\n",
    "        Result.append({\n",
    "            'Number': row['Number'],\n",
    "            'Statement': row['Statement'],\n",
    "            'Matched Statement': statement,\n",
    "            'Matched Document Reference': document,\n",
    "            'Similarity Score': score,\n",
    "            'Folder location': location\n",
    "        })\n",
    "\n",
    "# Print the Dataframe result\n",
    "output_df = pd.DataFrame(Result)\n",
    "#print(output_df)\n",
    "\n",
    "# Use pandas to create an Excel file with XlsxWriter module with similarity coloring base on three conditions\n",
    "output_file = 'Excel_file/Result.xlsx'\n",
    "with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "    # write a dataframe into an excel file\n",
    "    output_df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "    # Get the workbook and worksheet object\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "    \n",
    "    # Determine the cell range for the Similarity Score column (row 2 to the last row)\n",
    "    num_rows = len(output_df)\n",
    "    cell_range = f'E2:E{num_rows + 1}'\n",
    "    \n",
    "    # Apply conditional formatting:\n",
    "    # Red for scores below 80% (< 0.8)\n",
    "    red_format = workbook.add_format({'bg_color': '#FFC7CE', 'font_color': '#9C0006'})\n",
    "    worksheet.conditional_format(cell_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '<',\n",
    "        'value': 0.8,\n",
    "        'format': red_format\n",
    "    })\n",
    "    \n",
    "    # Orange for scores between 80% and 95% (0.8 to 0.95)\n",
    "    orange_format = workbook.add_format({'bg_color': '#FFEB9C', 'font_color': '#9C6500'})\n",
    "    worksheet.conditional_format(cell_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': 'between',\n",
    "        'minimum': 0.8,\n",
    "        'maximum': 0.95,\n",
    "        'format': orange_format\n",
    "    })\n",
    "    \n",
    "    # Green for scores 95% and above (>= 0.95)\n",
    "    green_format = workbook.add_format({'bg_color': '#C6EFCE', 'font_color': '#006100'})\n",
    "    worksheet.conditional_format(cell_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '>=',\n",
    "        'value': 0.95,\n",
    "        'format': green_format\n",
    "    })\n",
    "\n",
    "    # Set column width for each column\n",
    "    worksheet.set_column('A:A', 8)\n",
    "    worksheet.set_column('B:B', 50)\n",
    "    worksheet.set_column('C:C', 50)\n",
    "    worksheet.set_column('D:D', 65)\n",
    "    worksheet.set_column('F:F', 30)\n",
    "    \n",
    "    # column E format is set to percentage instead, round to 2 decimal point\n",
    "    percentage_format = workbook.add_format({'num_format': '0.00%'})\n",
    "    worksheet.set_column('E:E', 14, percentage_format)\n",
    "\n",
    "\n",
    "# Get end time and total time taken\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time: {total_time:.3f} seconds taken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing output folder...\n",
      "unlinking file: output\\1.14.1.pdf\n",
      "unlinking file: output\\1.14.2.pdf\n",
      "\n",
      "\n",
      "Opening PDF: ODB_13.pdf ...\n",
      "Annotation content updated to: 2.1\n",
      "Annotation content updated to: 2.1\n",
      "Annotation content updated to: 2.1\n",
      "Opening PDF: ODB_14.pdf ...\n",
      "Annotation content updated to: 2.2\n",
      "Annotation content updated to: 2.2\n",
      "Opening PDF: ODB_15.pdf ...\n",
      "Annotation content updated to: 2.3\n",
      "Opening PDF: ODB_16.pdf ...\n",
      "Annotation content updated to: 2.4\n",
      "Annotation content updated to: 2.4\n",
      "Annotation content updated to: 2.4\n",
      "Annotation content updated to: 2.4\n",
      "Annotation content updated to: 2.4\n",
      "Annotation content updated to: 2.4\n",
      "Opening PDF: ODB_17.pdf ...\n",
      "Annotation content updated to: 2.5\n",
      "Opening PDF: ODB_18.pdf ...\n",
      "Annotation content updated to: 2.6\n"
     ]
    }
   ],
   "source": [
    "# testing PDF editor with python (pip install PyMuPDF)\n",
    "import fitz\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Clear output folder\n",
    "output_dir = 'output'\n",
    "if os.path.exists(output_dir):\n",
    "    print(\"Clearing output folder...\")\n",
    "    # Go over the files and subdirectories in the output folder\n",
    "    for filename in os.listdir(output_dir):\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # remove file or link\n",
    "                print(\"unlinking file: \" + file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # remove directory and its contents\n",
    "                print(\"removing directory: \" + file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "else:\n",
    "    # Create the output folder if it does not exist\n",
    "    print(\"Making output folder...\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"\\n\")\n",
    "# this will be from the complier.py file (extracted from excel input file)\n",
    "# Here's a simple Map instead, replace this with actual columns from either the result.xlsx or dataframe column to map, number will be from the result.xlsx\n",
    "pdf_statements = {\n",
    "    \"ODB_13\": \"2.1\",\n",
    "    \"ODB_14\": \"2.2\",\n",
    "    \"ODB_15\": \"2.3\",\n",
    "    \"ODB_16\": \"2.4\",\n",
    "    \"ODB_17\": \"2.5\",\n",
    "    \"ODB_18\": \"2.6\",\n",
    "}\n",
    "# loop over each PDF file\n",
    "for saved_statement, cur_statement in pdf_statements.items():\n",
    "    print(f\"Opening PDF: {saved_statement}.pdf ...\")\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(f'document/{saved_statement}.pdf')\n",
    "\n",
    "    # loop over all pages in PDF file\n",
    "    for page in doc:\n",
    "        # loop over all annotations\n",
    "        for annot in page.annots():\n",
    "            # check if annotation is a freetext annotation\n",
    "            if \"FreeText\" in annot.type:\n",
    "                # Update annotation text\n",
    "                annot.set_info(content=cur_statement, title=\"Oracle\")\n",
    "                annot.update()  # save annotation\n",
    "\n",
    "                print(f\"Annotation content updated to: {cur_statement}\")\n",
    "\n",
    "    # Save the PDF file\n",
    "    doc.save(f'output/{cur_statement}.pdf')\n",
    "    doc.close()  # Close the document when done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start embedding main statements...\n",
      "['เป็นระบบจัดการฐานข้อมูลเชิงสัมพันธ์ (Relational Database Management System)', 'เป็นระบบจัดการฐานข้อมูลเชิงสัมพันธ์ที่สนับสนุนการทำงานแบบออปเจ็กต์ (Object-Relational Database Management System)', 'เป็นระบบจัดการฐานข้อมูลเชิงสัมพันธ์ที่สนับสนุนการทำงานแบบออบเจ็กต์ (Object-Relational Database Management System) โดยสามารถทำงานกับข้อมูลต่าง ๆ ดังนี้ ', 'สามารถเลือกทำงานบนระบบปฏิบัติการ Windows หรือ ระบบปฏิบัติการ UNIX', 'สามารถทำงานได้บนระบบปฎิบัติการ Linux และ Windows', 'สามารถติดตั้งบนระบบปฏิบัติการ Linux และระบบปฏิบัติการ Windows ได้เป็นอย่างน้อย', 'เป็นฐานข้อมูลที่มีระบบ Lock ข้อมูลในระดับ Row Level Locking จริง ๆ ซึ่ง Database Engine กระทำได้เอง โดยต้องไม่มีการเขียนโปรแกรมเพิ่มเติม', 'สามารถใช้งาน Lock ข้อมูลในระดับแถว (Row Level Locking) ได้อัตโนมัติ โดยไม่ต้องมีการพัฒนาโปรแกรมเพิ่มเติม', 'มีคุณสมบัติในการทำ Multi-Version Read Consistency โดยไม่มีการอ่านข้อมูลแบบ Dirty Reads ทั้งนี้เพื่อความถูกต้องของข้อมูลที่จะถูกนำไปใช้ ที่ซึ่งผู้เป็น Readers และ Writers ของข้อมูลจะต้องไม่ Block ซึ่งกันและกัน (ผู้อ่านข้อมูลไม่ Block ผู้เขียนข้อมูล และผู้เขียนข้อมูลไม่ Block ผู้อ่านข้อมูล)', 'สามารถให้ความสอดคล้องในการอ่านข้อมูล (Multi-Version Read Consistency) โดยไม่สร้างผลลัพธ์ที่ไม่ถูกต้อง (Dirty Reads) และไม่บล็อกการอ่านและเขียนข้อมูลของผู้ใช้อื่น', 'มีคุณสมบัติในการทำ Multi-Version Read Consistency โดยไม่มีการอ่านข้อมูลแบบ Dirty Reads ทั้งนี้เพื่อความถูกต้องของข้อมูลที่จะถูกนำไปใช้ โดย Readers และ Writers ของข้อมูลจะต้องไม่ block ซึ่งกันและกัน ', 'มีคุณสมบัติในการทำ Multi-Version Read Consistency โดยไม่มีการอ่านข้อมูลแบบ Dirty Reads ทั้งนี้เพื่อความถูกต้องของข้อมูลที่จะถูกนำไปใช้ ที่ซึ่งผู้เป็น Readers และ Writers ของข้อมูลจะต้องไม่ block ซึ่งกันและกัน (ผู้อ่านข้อมูลไม่ block ผู้เขียนข้อมูล และผู้เขียนข้อมูลไม่ block ผู้อ่านข้อมูล) ดังนั้นเมื่อมี transaction ที่ทำการเปลี่ยนแปลงข้อมูลแต่ยังไม่มีการ commit หรือ rollback ผู้ใช้งานอื่นจะต้องสามารถอ่านข้อมูลใน row นั้นได้ โดยเห็นข้อมูลของชุดก่อนที่จะมีการเปลี่ยนแปลงเนื่องจากข้อมูลดังกล่าวยังไม่มีการยืนยันการเปลี่ยนแปลงที่เป็นจริง', 'สามารถทำการเก็บข้อมูลและแสดงผลได้ทั้งภาษาไทยและภาษาอังกฤษ', 'มีการทำงานแบบ Machine Learning เพื่อช่วยเพิ่มประสิทธิภาพในการทำงานของระบบฐานข้อมูล', 'มีการทำงานแบบ Query Optimization และสามารถทำงานร่วมกับ Machine Learning เพื่อช่วยเพิ่มประสิทธิภาพในการทำงานของระบบฐานข้อมูล', 'สามารถรองรับการจัดเก็บข้อมูลในรูปแบบ JSON โดยสามารถค้นหาข้อมูลที่ถูกจัดเก็บในรูปแบบ JSON ได้', 'สามารถทำงานในรูปแบบระบบฐานข้อมูลแบบ Graph ได้', 'มีเครื่องมือรองรับในการจัดการระบบไฟล์สำหรับไฟล์ฐานข้อมูลโดยเฉพาะ โดยมีคุณสมบัติดังนี้', 'รองรับการช่วยกระจาย I/O ไปยังดิสก์ข้อมูลต่าง ๆ เพื่อเพิ่มประสิทธิภาพในการทำงาน', 'ช่วยกระจาย I/O ไปยังดิสก์ข้อมูลต่าง ๆ เพื่อเพิ่มประสิทธิภาพในการทำงาน และช่วยลดเรื่อง hotspots ของดิสก์ในฐานข้อมูล', 'รองรับการเพิ่มหรือลดจำนวน disk ได้โดยไม่กระทบกับระบบ', 'สามารถทำการเพิ่มดิสก์หรือลบดิสก์ออกจากฐานข้อมูลด้วยคำสั่ง SQL โดยไม่ต้องหยุดการทำงานของฐานข้อมูล และสามารถจัดการ Disk groups ด้วยเครื่องมือที่เป็น GUI ได้', 'รองรับการจัดเรียงการกระจายของข้อมูลใหม่ (Redistribution หรือ Rebalancing) ในกรณีที่มีการเพิ่มดิสก์หรือลบดิสก์ออกจากฐานข้อมูล', 'สามารถทำการจัดเรียงการกระจายของข้อมูลใหม่ (Redistribution หรือ Rebalancing) ในกรณีที่มีการเพิ่มดิสก์หรือลบดิสก์ออกจากฐานข้อมูล โดยการจัดเรียงดังกล่าวต้องทำงานในรูปแบบ background เพื่อให้กระทบกับประสิทธิภาพการทำงานของฐานข้อมูลน้อยที่สุด', 'รองรับการ Mirror Resync ข้อมูลระหว่าง Disk กรณีที่ Disk failed หรือไม่สามารถทำงานได้', 'สามารถทำการ Mirror Resync ข้อมูลระหว่าง Disk กรณีที่ Disk บางตัวมีการ Offline หรือไม่สามารถทำงานได้ โดยจะ Resync เฉพาะข้อมูลที่มีการเปลี่ยนแปลงระหว่าง Offline เท่านั้นเพื่อลดระยะเวลาในการทำงาน', 'รองรับการทำงานในลักษณะ Cluster (Active/Active)', 'มาพร้อมกับเครื่องมือในการสร้าง Web Application อย่างง่าย (Low-Code) โดยเครื่องมือนี้ต้องสนับสนุนการทำงานแบบ Web services และมีความสามารถในการทำ Drag and Drop สร้าง Chart ในรูปแบบต่าง ๆ ได้', 'สามารถทำงานแบบ Multi-Tenant ได้ไม่น้อยกว่า ๓ Tenant', 'ต้องสนับสนุน เน็ตเวิร์คโปรโตคอลแบบ TCP/IP เป็นอย่างน้อย', 'มีเครื่องมือที่ช่วยในการสร้างแอปพลิเคชันเว็บโดยใช้แนวคิด Low-Code ซึ่งสามารถสร้าง Web services และสร้างกราฟในรูปแบบ Drag and Drop ได้อย่างง่ายดาย', 'สามารถเลือกทำการ roll back กลับไป ณ ช่วงเวลาที่กำหนดได้ (point-in-time data recovery)', 'ระบบต้องมีความสามารถในการสร้าง Blockchain table เพื่อจัดเก็บข้อมูลที่มีความสำคัญซึ่งไม่สามารถแก้ไขเปลี่ยนแปลงได้ (insert only table) พร้อมกับ hash value ที่จะมีความสัมพันธ์กับรายการ (row) ก่อนหน้า โดยต้องสามารถตรวจสอบความถูกต้องของข้อมูล ในแต่ละรายการ (row) จาก hash value ที่ไม่ผ่านการแก้ไขได้', 'สามารถทำการเปลี่ยนแปลง Database Password ของ Application User ได้ โดยไม่ก่อให้เกิด Downtime ทำให้การทำงานของ Application ไม่ถูกกระทบ ขณะทำการเปลี่ยน Password', 'รองรับการสำเนาข้อมูล (Mirroring) ในระดับ file โดยรองรับการสำเนาทั้งใน รูปแบบสองสำเนา และสามสำเนา (2-Way Mirroring และ 3- Way Mirroring)', 'ข้อมูลเชิงสัมพันธ์ (Relational)', 'ข้อมูล Sharded', 'ข้อมูลเอกสาร (Document Store) เช่น JSON, XML, Text ', 'ข้อมูลเชิงแผนที่ (Spatial)', 'ข้อมูล Graph และ Triple Store', 'สามารถทำการสำรองและกู้ข้อมูล (Database Backup/Restore) ได้ในลักษณะต่อไปนี้', 'Full Database Backup', 'Incremental Backup', 'Online Backup', 'สามารถสนับสนุนการทำงานแบบ Parallel โดยมีคุณสมบัติดังนี้', 'Parallel Statistics Gathering', 'Parallel Index Build/Scans', 'Parallel Data Pump Export/Import', 'Parallel Query/DML', 'Parallel Backup and Recovery', 'มีเครื่องมือในการสร้าง Web Application ในลักษณะ Low Coding ที่สามารถสร้างหน้าจอ ', 'Form สำหรับเพิ่ม ปรับปรุง และลบข้อมูล', 'สามารถสร้างหน้าจอจัดการข้อมูลในลักษณะ Grid ', 'สามารถเชื่อมโยงข้อมูลผ่าน REST API', 'สามารถสร้าง Chart ต่างๆ ที่รองการแสดงผ่าน HTML5 เป็น Responsive ', 'เป็นระบบ Data Mining ที่ embedded อยู่ในระบบ Data Service Platform เพื่อลดการเคลื่อนย้ายข้อมูลออกจากฐานข้อมูล', 'สามารถพัฒนา โดยใช้ SQL, Stored Procedures ,PL/SQL Server และ Trigger ได้', 'สามารถรองรับลักษณะงานที่เป็น Analytic workload โดยมีคุณสมบัติดังนี้', 'สามารถจัดเก็บข้อมูลในรูปแบบ Columnar data store', 'สามารถสร้างตาราง (Table) ในรูปแบบ Row-based และ Column-based ในฐานข้อมูลเดียวกัน', 'สามารถประมวลผล Query ได้อย่างมีประสิทธิภาพโดยไม่ต้องสร้าง Indexes', 'สามารถประมวลผลในรูปแบบที่เป็น massively parallel processing (MPP) หรือparallel execution', 'สามารถทำงานในรูปแบบ Cluster Database แบบ Active-Active หรือ Active-Passive ได้ และ ต้องสามารถทำงานแบบ Multi-Tenant ได้โดยต้องมีลิขสิทธิ์การทำงานไม่ต่ำกว่า 3 tenants', 'สามารถทำงานได้บนระบบปฏิบัติการ UNIX, Linux และ Windows ได้เป็นอย่างน้อย', 'สามารถเชื่อมต่อฐานข้อมูลด้วยเน็ตเวิร์คโพรโทคอล TCP/IP , HTTP , WebDAV และ FTP ได้เป็นอย่างน้อย', 'สามารถเชื่อมต่อฐานข้อมูลด้วยเน็ตเวิร์คโพรโทคอล TCP/IP , HTTP , WebDAV และ FTP ได้เป็นอย่างน้อย', 'สามารถเชื่อมต่อฐานข้อมูลด้วยมาตรฐาน', 'Open Database Connectivity (ODBC)', 'Java Database Connectivity (JDBC)', 'Net, PHP, Python, C, C++', 'สามารถรองรับการทำงานกับข้อมูลรูปแบบ Character , Variable Character , Numeric , Date , BLOB , XML และ JSON ได้เป็นอย่างน้อย', 'สนับสนุนการทำ Index, Referential Integrity, Unique Constraints', 'สนับสนุนการทำ View, Stored Procedure, Function และ Trigger ']\n"
     ]
    }
   ],
   "source": [
    "# test why cannot embeded\n",
    "import pandas as pd\n",
    "df_main = pd.read_excel('Excel_file/ODB_Mapped_PDF.xlsx')\n",
    "print(\"Start embedding main statements...\")\n",
    "main_statements = df_main['Statement'].tolist()\n",
    "#main_embeddings = model.encode(main_statements, convert_to_tensor=True, show_progress_bar=True)\n",
    "print(main_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.1</td>\n",
       "      <td>\\tเป็นระบบจัดเก็บฐานข้อมูลเชิงสัมพันธ์ที่สนับส...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.2</td>\n",
       "      <td>สามารถทำงานได้บนระบบปฏิบัติการ ได้แก่ Linux แล...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.3</td>\n",
       "      <td>สามารถเชื่อมต่อฐานข้อมูลด้วยเน็ตเวิร์คโพรโทคอล...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.4</td>\n",
       "      <td>สามารถเชื่อมต่อฐานข้อมูลด้วยมาตรฐาน Open Datab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.5</td>\n",
       "      <td>สามารถรองรับการทำงานกับข้อมูลรูปแบบ Character ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.6</td>\n",
       "      <td>สนับสนุนการทำ Index, Referential Integrity, Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.7</td>\n",
       "      <td>สนับสนุนการทำ View, Stored Procedure, Function...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.8</td>\n",
       "      <td>มีระบบช่วยเหลือในการสืบค้นข้อมูล (Query Optimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.9</td>\n",
       "      <td>มีคุณสมบัติในการทำ Multi-Version Read Consiste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.10</td>\n",
       "      <td>สามารถสำรองและกู้คืนฐานข้อมูล (Database Backup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.11</td>\n",
       "      <td>สามารถทำการเก็บข้อมูลและแสดงผลได้ทั้งภาษาไทยแล...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.12</td>\n",
       "      <td>มีเครื่องมือในการสร้าง Web Application อย่างง่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.13</td>\n",
       "      <td>รองรับทำการสร้าง Machine Learning Model โดยมี ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.14</td>\n",
       "      <td>รองรับทำการสร้าง Database แบบ Multitenant โดยม...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.15</td>\n",
       "      <td>รองรับการทำ High Availability ทำงานได้แบบ Acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.1</td>\n",
       "      <td>สามารถจำกัดสิทธิ์ของผู้ดูแลฐานข้อมูล (Database...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9.2</td>\n",
       "      <td>สามารถกำหนดช่วงเวลาในการทำงานของ DBA และผู้ใช้...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9.3</td>\n",
       "      <td>สามารถจำกัดสิทธิ์ในการเปลี่ยนสิทธ์ของผู้ดูแลระ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9.4</td>\n",
       "      <td>สามารถเชื่อมต่อกับระบบ Monitoring ต่างๆ ได้ เช...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.5</td>\n",
       "      <td>สามารถทำการเข้ารหัสข้อมูลในระดับตาราง (Tables)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number                                          Statement\n",
       "0     3.1  \\tเป็นระบบจัดเก็บฐานข้อมูลเชิงสัมพันธ์ที่สนับส...\n",
       "1     3.2  สามารถทำงานได้บนระบบปฏิบัติการ ได้แก่ Linux แล...\n",
       "2     3.3  สามารถเชื่อมต่อฐานข้อมูลด้วยเน็ตเวิร์คโพรโทคอล...\n",
       "3     3.4  สามารถเชื่อมต่อฐานข้อมูลด้วยมาตรฐาน Open Datab...\n",
       "4     3.5  สามารถรองรับการทำงานกับข้อมูลรูปแบบ Character ...\n",
       "5     3.6  สนับสนุนการทำ Index, Referential Integrity, Un...\n",
       "6     3.7  สนับสนุนการทำ View, Stored Procedure, Function...\n",
       "7     3.8  มีระบบช่วยเหลือในการสืบค้นข้อมูล (Query Optimi...\n",
       "8     3.9  มีคุณสมบัติในการทำ Multi-Version Read Consiste...\n",
       "9    3.10  สามารถสำรองและกู้คืนฐานข้อมูล (Database Backup...\n",
       "10   3.11  สามารถทำการเก็บข้อมูลและแสดงผลได้ทั้งภาษาไทยแล...\n",
       "11   3.12  มีเครื่องมือในการสร้าง Web Application อย่างง่...\n",
       "12   3.13  รองรับทำการสร้าง Machine Learning Model โดยมี ...\n",
       "13   3.14  รองรับทำการสร้าง Database แบบ Multitenant โดยม...\n",
       "14   3.15  รองรับการทำ High Availability ทำงานได้แบบ Acti...\n",
       "15    9.1  สามารถจำกัดสิทธิ์ของผู้ดูแลฐานข้อมูล (Database...\n",
       "16    9.2  สามารถกำหนดช่วงเวลาในการทำงานของ DBA และผู้ใช้...\n",
       "17    9.3  สามารถจำกัดสิทธิ์ในการเปลี่ยนสิทธ์ของผู้ดูแลระ...\n",
       "18    9.4  สามารถเชื่อมต่อกับระบบ Monitoring ต่างๆ ได้ เช...\n",
       "19    9.5  สามารถทำการเข้ารหัสข้อมูลในระดับตาราง (Tables)..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dealing with zeros -> make sure number col is in \"text\" format and trailing zero is correct\n",
    "import pandas as pd\n",
    "df_main = pd.read_excel(\n",
    "    'Excel_file/Compare.xlsx',\n",
    "    sheet_name=\"ODB\",\n",
    "    header=1,\n",
    "    dtype={'Number': str}  # Force 'Number' column to be read as text\n",
    ")\n",
    "df_main.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the model using the ONNX backend and ensure export is enabled.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparaphrase-multilingual-MiniLM-L12-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexport\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:308\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[0;32m    299\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[0;32m    302\u001b[0m     model_name_or_path,\n\u001b[0;32m    303\u001b[0m     token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    307\u001b[0m ):\n\u001b[1;32m--> 308\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[0;32m    321\u001b[0m         model_name_or_path,\n\u001b[0;32m    322\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    330\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1739\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1741\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(model_name_or_path)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:81\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[0;32m     78\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     80\u001b[0m config, is_peft_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[0;32m     84\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:188\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[1;34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_peft_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_only_kwargs)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_onnx_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenvino\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_openvino_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:290\u001b[0m, in \u001b[0;36mTransformer._load_onnx_model\u001b[1;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[0;32m    287\u001b[0m     model_args\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Either load an exported model, or export the model to ONNX\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model: ORTModelForFeatureExtraction \u001b[38;5;241m=\u001b[39m \u001b[43mORTModelForFeatureExtraction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Wrap the save_pretrained method to save the model in the correct subfolder\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model\u001b[38;5;241m.\u001b[39m_save_pretrained \u001b[38;5;241m=\u001b[39m _save_pretrained_wrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model\u001b[38;5;241m.\u001b[39m_save_pretrained, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\optimum\\onnxruntime\\modeling_ort.py:734\u001b[0m, in \u001b[0;36mORTModel.from_pretrained\u001b[1;34m(cls, model_id, export, force_download, use_auth_token, token, cache_dir, subfolder, config, local_files_only, provider, session_options, provider_options, use_io_binding, **kwargs)\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot use both `use_auth_token` and `token` arguments at the same time.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    732\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m--> 734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_io_binding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_io_binding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\optimum\\modeling_base.py:383\u001b[0m, in \u001b[0;36mOptimizedModel.from_pretrained\u001b[1;34m(cls, model_id, export, force_download, use_auth_token, token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    379\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument `revision` was set to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but will be ignored for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    380\u001b[0m         )\n\u001b[0;32m    381\u001b[0m     model_id, revision \u001b[38;5;241m=\u001b[39m model_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 383\u001b[0m all_files, _ \u001b[38;5;241m=\u001b[39m \u001b[43mTasksManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m config_folder \u001b[38;5;241m=\u001b[39m subfolder\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_files:\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\optimum\\exporters\\tasks.py:1611\u001b[0m, in \u001b[0;36mTasksManager.get_model_files\u001b[1;34m(model_name_or_path, subfolder, cache_dir, use_auth_token, token, revision)\u001b[0m\n\u001b[0;32m   1609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_name_or_path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1610\u001b[0m     model_name_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(model_name_or_path)\n\u001b[1;32m-> 1611\u001b[0m all_files \u001b[38;5;241m=\u001b[39m \u001b[43mhuggingface_hub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_repo_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subfolder \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1618\u001b[0m     all_files \u001b[38;5;241m=\u001b[39m [file[\u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :] \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m all_files \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mstartswith(subfolder)]\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\huggingface_hub\\hf_api.py:2891\u001b[0m, in \u001b[0;36mHfApi.list_repo_files\u001b[1;34m(self, repo_id, revision, repo_type, token)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[0;32m   2863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlist_repo_files\u001b[39m(\n\u001b[0;32m   2864\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2869\u001b[0m     token: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2870\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   2871\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2872\u001b[0m \u001b[38;5;124;03m    Get the list of files in a given repo.\u001b[39;00m\n\u001b[0;32m   2873\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2889\u001b[0m \u001b[38;5;124;03m        `List[str]`: the list of files in a given repository.\u001b[39;00m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[0;32m   2892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrfilename\u001b[49m\n\u001b[0;32m   2893\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_repo_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   2895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2896\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRepoFile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\huggingface_hub\\hf_api.py:3028\u001b[0m, in \u001b[0;36mHfApi.list_repo_tree\u001b[1;34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[0m\n\u001b[0;32m   3026\u001b[0m encoded_path_in_repo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m quote(path_in_repo, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m path_in_repo \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3027\u001b[0m tree_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mencoded_path_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 3028\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaginate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecursive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3029\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mRepoFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpath_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mRepoFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpath_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\huggingface_hub\\utils\\_pagination.py:36\u001b[0m, in \u001b[0;36mpaginate\u001b[1;34m(path, params, headers)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fetch a list of models/datasets/spaces and paginate through results.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03mThis is using the same \"Link\" header format as GitHub.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m- https://docs.github.com/en/rest/guides/traversing-with-pagination#link-header\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m session \u001b[38;5;241m=\u001b[39m get_session()\n\u001b[1;32m---> 36\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Saving as a local modal to ONNX\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the model using the ONNX backend and ensure export is enabled.\n",
    "model = SentenceTransformer(\n",
    "    \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    backend=\"onnx\",\n",
    "    model_kwargs={\"export\": True}\n",
    ")\n",
    "\n",
    "# Save the model locally; this will create the correct folder structure.\n",
    "model.save_pretrained(\"./onnx_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Autocomply\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** EP Error ***************\n",
      "EP Error D:\\a\\_work\\1\\s\\onnxruntime\\python\\onnxruntime_pybind_state.cc:505 onnxruntime::python::RegisterTensorRTPluginsAsCustomOps Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider']\n",
      "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
      "****************************************\n",
      "Cosine similarity: 0.7313246130943298\n"
     ]
    }
   ],
   "source": [
    "# test out local model\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer(\n",
    "    \"./onnx_model\",\n",
    "    backend=\"onnx\",\n",
    "    model_kwargs={\"file_name\": \"model.onnx\"}\n",
    ")\n",
    "\n",
    "# Define the two sentences\n",
    "sentence1 = 'Im very happy right now, so thats the best right'\n",
    "sentence2 = 'Im very sad right now, so thats the best right'\n",
    "\n",
    "# Generate embeddings for each sentence\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity between the embeddings\n",
    "cosine_score = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(\"Cosine similarity:\", cosine_score.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
